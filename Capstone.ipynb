{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Analysis of Repetition in Teaching\"\n",
    "subtitle: \"Authors: Nastaran Mesgari\"\n",
    "format:\n",
    "  html:                     \n",
    "    standalone: true        \n",
    "    embed-resources: true   \n",
    "    code-fold: true        \n",
    "    number-sections: true  \n",
    "    toc: true \n",
    "               \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exclusive Summery\n",
    "In order to find duplications and raise the standard of the curriculum overall, this project focuses on comparing the content of two courses—AI and visualization. This was accomplished by taking a methodical approach. Two courses were chosen: one on artificial intelligence, which covered subjects like data models, algorithms, and machine learning; the other on visualization, which placed an emphasis on graphical data representation, storytelling, and insight generation. For each course, a set of domain-specific keywords wrere created in order to finds and quantify content overlaps. \"AI,\" \"machine,\" \"learning,\" \"intelligence,\" \"algorithm,\" and \"data\" were among the keywords related to AI, while \"story,\" \"narrative,\" \"visualization,\" \"insights,\" and \"emotion\" were among the keywords related to visualization.  \n",
    "\n",
    "To guarantee accurate analysis, the course materials were preprocessed, which included eliminating unnecessary text, changing to lowercase, and transforming the data into formats that are easier to handle.**82 AI-related keywords and 27 visualization-related keywords** were found in the AI course, according to a keyword frequency analysis.  there were 216 keywords related to visualization and 123 related to AI in the visualization course.  similarity score of **0.2433** was obtained from this overlaps, suggesting a moderate degree of redundancy between the two courses. To further show the distribution of keyword and give a visual depiction of the most common terms in the materials, word clouds were created for both courses. \n",
    "\n",
    "The results indicate that although the courses retain their distinctt , there are significant conceptual overlaps, especially in fields pertaining to data and model application. This overlap suggests possible duplications that, if resolved, could improve the uniqueness and efficacy of  each course. \n",
    "In order to find more extensive redundancy patterns throughout the curriculuom, the analysis could be extended to include more courses as a subsequent step. To learns more about content overlaps, advanced Natural Language Processing (NLP) method  as like as topic modeling and similarity analysis could be used. These results could guide focused content optimization tacti, guaranteeing a distinct distinction between courses and enhancing students' overall educational experience. To improve course content, reduce duplication, and develop a more coherent curriculum, cooperation with instructors is advised. \n",
    "\n",
    "This project lays the groundwork for future initiatives to maximize academic programs by highlighting the significance of assessing instructional materials for alignment and redundancy. University can improve learning outcomes, encourage innovation, and preserve student learning by making sure that courses are streamlined and complementary.The insights gained from this analysis are instrumental in achieving these goals and improving the quality of teaching and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instroduction\n",
    "In academic settings, the quality and structure of course content significantly impact students' learning experiences and outcomes. With the growing importance of fields like Artificial Intelligence (AI) and Data Visualization, the need for well-structured and non-redundant educational programs has become increasingly critical. Redundant content, while sometimes unavoidable, can lead to inefficiencies in learning, reduce engagement, and limit students' exposure to new concepts. Addressing such overlaps is essential for optimizing course offerings and ensuring students receive a comprehensive yet streamlined education.  \n",
    "\n",
    "This project aims to analyze and evaluate the extent of content similarities between two courses—one focusing on AI and the other on Visualization. These domains, while distinct, often intersect in their discussion of data-driven decision-making and the application of models to real-world problems. By systematically identifying overlaps, the project seeks to highlight areas of potential redundancy and offer solutions for improving course alignment.  \n",
    "\n",
    "The analysis is centered on identifying keywords representative of the core themes in each course. Preprocessing techniques are applied to refine the course materials for accurate assessment, followed by similarity metrics to quantify the degree of overlap. The results are visualized through tools such as Word Clouds, providing a clear picture of the shared and unique elements of the courses.  \n",
    "\n",
    "This initiative not only supports the academic goals of content optimization but also fosters collaboration among instructors. By aligning course objectives and minimizing redundancies, educators can offer more diverse and impactful learning experiences. This project serves as a stepping stone for broader efforts to evaluate and enhance curriculum design, ultimately contributing to higher standards in education and better preparing students for the challenges of an ever-evolving professional landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out this project, a step-by-step approach was followed to properly analyze the content similarity between two courses related to \"Artificial Intelligence\" and \"Data Visualization.\" These steps included data identification, text processing, feature extraction, and similarity analysis. Below is a breakdown of each step:\n",
    "\n",
    "**1. Course Selection**\n",
    "Initially, two courses related to \"Artificial Intelligence\" and \"Data Visualization\" were chosen as the subjects for the similarity analysis. For each of these courses, the corresponding text files were extracted and prepared for processing:\n",
    "\n",
    "Course 1: AI2.docx (Content related to Artificial Intelligence)\n",
    "Course 2: Vir1.docx (Content related to Data Visualization)\n",
    "\n",
    "**2. Definition of Key Terms**\n",
    "Next, a set of key terms related to the topics of each course was defined. These terms were used to identify and compare the similarities between the courses:\n",
    "\n",
    "For the AI course: {'ai', 'machine', 'learning', 'intelligence', 'algorithm', 'data', 'model'}\n",
    "For the Data Visualization course: {'story', 'narrative', 'data', 'visualization', 'insights', 'message', 'emotion'}\n",
    "\n",
    "**3. Text Processing and Cleaning**\n",
    "The content of both text files was processed to remove any noise and irrelevant data. This step involved converting the text into a standard format, making it ready for analysis to ensure the results were accurate.\n",
    "\n",
    "**4. Keyword Analysis**\n",
    "At this stage, the number of key terms found in each course was counted. This count served as a metric for comparing the similarities and differences:\n",
    "\n",
    "In Course 1: 82 AI-related terms and 27 Data Visualization-related terms.\n",
    "In Course 2: 123 AI-related terms and 216 Data Visualization-related terms.\n",
    "\n",
    "**5. Similarity Calculation**\n",
    "Using basic formulas, the similarity between the two courses was calculated. One of the metrics used was the comparison of the frequency of key terms in each course, resulting in a similarity score of 0.2433. This indicates a meaningful degree of similarity between the two courses, suggesting there may be overlap in the teaching of certain concepts.\n",
    "\n",
    "**6. Word Cloud Generation**\n",
    "A Word Cloud was created for each course, visually representing the distribution of key terms in their content. This visualization aids in better understanding the similarities and differences between the courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the first I import my necessary library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import load_files\n",
    "# nltk.download('stopwords')\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import keras\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, average_precision_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and Cleaning\n",
    "In any text analysis task, the first crucial step is text processing and cleaning. This stage ensures that the raw textual data is ready for further analysis, such as keyword extraction, sentiment analysis, or similarity detection. During text processing, we remove unwanted characters, irrelevant information, and perform normalization, so the text can be analyzed in a consistent and structured manner.\n",
    "\n",
    "In this project, text processing and cleaning involved extracting content from .docx files, removing unnecessary numerical data from filenames, and preparing the content by structuring it into a usable format for analysis. This process is vital to ensure that only the relevant text data is considered in subsequent steps of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: AI2.docx - Label: AI\n",
      "Content: \n",
      "OK. So that's a little bit the results. OK, so we are somewhere between intermediate and beginners.\n",
      "Great that you feel confident that you might learn the AI concepts. That's very good to know, and o...\n",
      "----------------------------------------\n",
      "Filename: Vir1.docx - Label: Vir\n",
      "Content: \n",
      "Mine too, but last week was I was in Oxford for a seminar and had to prepare the courses.\n",
      "So this was quite stressful and it seems this semester for the new students is even more confusion with getti...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Extracting Content and Labels from DOCX Files\n",
    "import os\n",
    "import docx\n",
    "import re\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to remove numbers from the filename and assign a label\n",
    "def get_label_from_filename(filename):\n",
    "    # Remove digits from the filename using regex\n",
    "    clean_name = re.sub(r'\\d+', '', filename)\n",
    "    # Remove file extension (.docx)\n",
    "    clean_name = clean_name.replace('.docx', '').strip()\n",
    "    return clean_name\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read all docx files from the directory and assign a label based on the filename\n",
    "texts = []\n",
    "labels = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(text)\n",
    "        filenames.append(filename)\n",
    "        \n",
    "        # Extract label from the filename without numbers\n",
    "        label = get_label_from_filename(filename)\n",
    "        labels.append(label)\n",
    "\n",
    "# Display results\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Filename: {filenames[i]} - Label: {labels[i]}\")\n",
    "    print(f\"Content: {text[:200]}...\")  # Display the first 200 characters of the text for brevity\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we performed several preprocessing steps to prepare the text data for similarity analysis. The preprocessing steps we implemented include:\n",
    "**(Similarity between AI2.docx and Vir1.docx: 0.8910 and it is too much)**\n",
    "\n",
    "Converting text to lowercase: This step ensured that text comparison was case-insensitive.\n",
    "\n",
    "Removing numbers: Digits were removed to focus solely on the textual content.\n",
    "\n",
    "Removing punctuation: This helped in standardizing the text and reducing noise.\n",
    "\n",
    "While these preprocessing steps were helpful, we identified areas where improvements were necessary to achieve a more accurate similarity assessment. To enhance our approach, we implemented the following additional steps:\n",
    "\n",
    "Removing stop words: Commonly used words like \"the,\" \"and,\" or \"is,\" which do not contribute significantly to the meaning, were removed to reduce their impact on the similarity calculation.\n",
    "\n",
    "Applying lemmatization or stemming: This step involved reducing words to their base or root form (e.g., \"running\" to \"run\"), allowing us to treat different forms of the same word as identical.\n",
    "\n",
    "Focusing on key terms: We prioritized words related to the specific topics of interest (e.g., \"AI,\" \"algorithm,\" \"model,\" \"visualization\") to limit the analysis to relevant content.\n",
    "\n",
    "Handling repetitive sections: Repeated phrases or structural similarities across documents were addressed to prevent inflated similarity scores.\n",
    "Exploring semantic similarity: Advanced methods, such as using semantic models (e.g., BERT or Word2Vec), were considered to account for contextual meanings rather than solely relying on word matching.\n",
    "\n",
    "These additional steps were crucial in refining the preprocessing process and ensuring that the similarity analysis yielded meaningful and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between AI2.docx and Vir1.docx: 0.8910\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation and convert to lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# List to store the text content of the files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "# Read all docx files from the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Compute TF-IDF vectors for the texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Compute cosine similarity between the first two documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display cosine similarity between the files\n",
    "for i, filename in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"Similarity between {filenames[i]} and {filenames[j]}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity using LDA:\n",
      "Similarity between AI2.docx and Vir1.docx: 0.4458\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation and convert to lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# List to store the text content of the files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "# Read all docx files from the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# --- Step 1: Compute TF-IDF vectors for the texts ---\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# --- Step 2: Apply Latent Dirichlet Allocation (LDA) ---\n",
    "n_topics = 40  # Specify the number of topics you want to extract (can be adjusted)\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_matrix = lda.fit_transform(tfidf_matrix)  # Transform TF-IDF matrix into topic distribution\n",
    "\n",
    "# --- Step 3: Compute cosine similarity using the LDA matrix ---\n",
    "similarity_matrix_lda = cosine_similarity(lda_matrix)\n",
    "\n",
    "# --- Step 4: Display cosine similarity between the files based on LDA ---\n",
    "print(\"\\nCosine Similarity using LDA:\")\n",
    "for i, filename1 in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity_lda = similarity_matrix_lda[i, j]\n",
    "        print(f\"Similarity between {filenames[i]} and {filenames[j]}: {similarity_lda:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity using TF-IDF:\n",
      "Similarity between AI2.docx and Vir1.docx: 0.4907\n",
      "\n",
      "Cosine Similarity using LDA:\n",
      "Similarity between AI2.docx and Vir1.docx: 0.4458\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation and convert to lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# List to store the text content of the files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "# Read all docx files from the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# --- Step 1: Compute TF-IDF vectors for the texts ---\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# --- Step 2: Compute cosine similarity using TF-IDF ---\n",
    "similarity_matrix_tfidf = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# --- Step 3: Compute LDA ---\n",
    "# Specify number of topics (n_components)\n",
    "n_topics = 40\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_matrix = lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# --- Step 4: Compute cosine similarity using LDA ---\n",
    "similarity_matrix_lda = cosine_similarity(lda_matrix)\n",
    "\n",
    "# --- Step 5: Display results for comparison ---\n",
    "# Display cosine similarity between the files using TF-IDF\n",
    "print(\"\\nCosine Similarity using TF-IDF:\")\n",
    "for i, filename1 in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity_tfidf = similarity_matrix_tfidf[i, j]\n",
    "        print(f\"Similarity between {filenames[i]} and {filenames[j]}: {similarity_tfidf:.4f}\")\n",
    "\n",
    "# Display cosine similarity between the files using LDA\n",
    "print(\"\\nCosine Similarity using LDA:\")\n",
    "for i, filename1 in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity_lda = similarity_matrix_lda[i, j]\n",
    "        print(f\"Similarity between {filenames[i]} and {filenames[j]}: {similarity_lda:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this code is to compare two text documents to identify **common words** and **common bigrams (sequences of two consecutive words)** between them. The code aims to provide a better understanding of the overlap and similarity between the two documents. The main steps and objectives are as follows:\n",
    "\n",
    "1. **Reading and Preprocessing Files:**\n",
    "   - Text files (in `.docx` format) are read from a specified directory.\n",
    "   - The content of each file is extracted and preprocessed:\n",
    "     - Converted to lowercase (case normalization).\n",
    "     - Numbers and punctuation are removed.\n",
    "   - The goal of preprocessing is to standardize the text for more accurate analysis.\n",
    "\n",
    "2. **Tokenizing the Text:**\n",
    "   - The text of each file is split into a list of words (tokens).\n",
    "   - This step enables the identification of repeated and common words.\n",
    "\n",
    "3. **Finding Common Words and Calculating Frequency:**\n",
    "   - Common words between the two documents are identified using set intersection (`&`).\n",
    "   - The frequency of each common word in both files is calculated and displayed.\n",
    "\n",
    "4. **Identifying Common Bigrams (Two Consecutive Words):**\n",
    "   - Each text is transformed into a sequence of bigrams (pairs of consecutive words).\n",
    "   - Common bigrams between the two documents are extracted and displayed.\n",
    "   - Examining bigrams helps identify recurring structures or phrases between the texts.\n",
    "\n",
    "\n",
    "The code is designed to analyze the similarity between two text documents in terms of content and structure. The results can be used to evaluate thematic overlap, detect potential repetitions, or refine textual content.\n",
    "\n",
    "## **Reasons for High Similarities:**\n",
    "If there are many common words or bigrams between the documents, it may be due to the following reasons:\n",
    "1. **Similar Topics:** Both documents might cover the same subject matter.\n",
    "2. **Repetitive Phrases:** Standardized phrases or templates may have been used in both files.\n",
    "3. **Insufficient Preprocessing:** Non-essential words (e.g., \"the,\" \"is\") were not removed, affecting the similarity results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between AI2.docx and Vir1.docx: 0.4907\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS  # for removing stopwords\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation, convert to lowercase, and remove stopwords)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read all docx files from the directory\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Initialize the TfidfVectorizer with stop words removed\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Using 'english' as stop words\n",
    "\n",
    "# Convert texts to TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Compute cosine similarity between the TF-IDF vectors\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display cosine similarity between the files\n",
    "for i, filename1 in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"Cosine Similarity between {filenames[i]} and {filenames[j]}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between AI2.docx and Vir1.docx using LDA: 0.4004\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS  # for removing stopwords\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation, convert to lowercase, and remove stopwords)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read all docx files from the directory\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Initialize the TfidfVectorizer with stop words removed and use n-grams (bi-grams)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))  # ngram_range=(1, 2) to include unigrams and bigrams\n",
    "\n",
    "# Convert texts to TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA) with more topics (increase n_components)\n",
    "lda_model = LatentDirichletAllocation(n_components=40, random_state=42)  # Increase the number of topics\n",
    "\n",
    "# Fit LDA model to the TF-IDF matrix\n",
    "lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Compute cosine similarity between the LDA topic distributions of the documents\n",
    "similarity_matrix = cosine_similarity(lda_matrix)\n",
    "\n",
    "# Display cosine similarity between the files\n",
    "for i, filename1 in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"Cosine Similarity between {filenames[i]} and {filenames[j]} using LDA: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What This Code Shows:**\n",
    "\n",
    "This code applies **Latent Dirichlet Allocation (LDA)**, a topic modeling technique, to identify the underlying topics within the provided `.docx` files. The output, which lists the **top words for each topic**, reveals the key terms most strongly associated with each identified topic.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **Topic 1:** ['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
    "- **Topic 2:** ['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']\n",
    "\n",
    "These words represent the **essence of the topics** extracted by the model from the provided documents. \n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose and Use for Subsequent Steps:**\n",
    "\n",
    "1. **Understanding Textual Themes:**\n",
    "   - The output helps us understand the primary themes or topics present in the documents. For instance:\n",
    "     - **Topic 1** seems to focus on conversational terms, possibly indicating informal communication or brainstorming.\n",
    "     - **Topic 2** includes terms like \"AI\" and \"decision,\" which might suggest discussions related to artificial intelligence and decision-making.\n",
    "\n",
    "2. **Refining Text Processing Pipelines:**\n",
    "   - By analyzing these topics, we can identify noise in the text (e.g., filler words like \"yeah,\" \"maybe,\" \"ok,\" and \"just\").\n",
    "   - This insight can guide further preprocessing, such as customizing the stop-word list to exclude irrelevant terms and focus on more meaningful content.\n",
    "\n",
    "3. **Basis for Text Classification or Comparison:**\n",
    "   - Extracted topics can serve as features for comparing documents, grouping similar texts, or even training classification models if labels are available.\n",
    "\n",
    "4. **Improving Alignment With Previous Results:**\n",
    "   - In earlier steps, we identified word frequencies, common words, and bigrams. This approach builds on those analyses by summarizing the **conceptual structure** of the text, helping to better understand similarities and differences between documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use This After Previous Code:**\n",
    "- The previous steps analyzed **word-level overlap** (e.g., shared words and phrases). However, they did not provide a high-level understanding of the themes or topics.\n",
    "- LDA adds a layer of abstraction by grouping related words into topics, giving a clearer picture of the documents' **semantic content**.\n",
    "- This approach ensures we’re not just comparing superficial textual similarities but also deeper thematic alignments, which is crucial for making informed decisions in downstream tasks.\n",
    "\n",
    " this code helps identify the main topics in the documents, providing a thematic overview that can be used to refine the preprocessing pipeline and guide further analyses. but aswe  can see the resault is not good and we decided change the code and check the other ways.\n",
    "\n",
    " Topic 1:\n",
    "['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
    "Topic 2:\n",
    "['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
      "Topic 2:\n",
      "['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to read and preprocess docx content\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Assuming 2 main topics\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparison Between the Two Codes**\n",
    "\n",
    "Both codes aim to extract **topics** from `.docx` files using **Latent Dirichlet Allocation (LDA)**, but there are significant differences in their **preprocessing steps** and the resulting outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences:**\n",
    "\n",
    "1. **Custom Stopwords:**\n",
    "   - **Previous Code:** Relied only on the built-in English stopword list provided by `CountVectorizer`.\n",
    "   - **Current Code:** Adds a **custom stopword list** (`['yeah', 'ok', 'umm', 'just', 'like', 'bit', 'maybe', 'right', 'thats']`) to filter out common filler words that do not contribute to meaningful topics.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - **Previous Code:** \n",
    "     - Lowercased text.\n",
    "     - Removed digits and punctuation.\n",
    "   - **Current Code:** \n",
    "     - Includes all previous preprocessing steps.\n",
    "     - Additionally removes **custom filler words**, making the data cleaner and more topic-focused.\n",
    "\n",
    "3. **Output Topics:**\n",
    "   - **Previous Code:**\n",
    "     - **Topic 1:** ['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
    "     - **Topic 2:** ['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']\n",
    "     - These topics contain numerous filler words (e.g., \"yeah,\" \"ok,\" \"maybe\") that obscure the main themes.\n",
    "   - **Current Code:**\n",
    "     - **Topic 1:** ['ai', 'things', 'probably', 'say', 'decision', 'kind', 'human', 'actually', 'basically', 'different']\n",
    "     - **Topic 2:** ['think', 'data', 'lets', 'know', 'say', 'income', 'dont', 'course', 'really', 'want']\n",
    "     - The removal of filler words results in more **coherent topics**, highlighting terms related to artificial intelligence, decision-making, and data.\n",
    "\n",
    "4. **Clarity of Topics:**\n",
    "   - The **current code** generates more focused topics, making them easier to interpret and align with the context of the documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Results:**\n",
    "\n",
    "### **Previous Results:**\n",
    "- Contained many **irrelevant filler words**.\n",
    "- Topic differentiation was less clear due to noise in the data.\n",
    "\n",
    "### **Current Results:**\n",
    "- More meaningful and specific:\n",
    "  - **Topic 1** emphasizes AI-related terms like \"ai,\" \"decision,\" and \"different.\"\n",
    "  - **Topic 2** relates to discussions about data analysis, with terms like \"data,\" \"income,\" and \"course.\"\n",
    "- The removal of custom stopwords improved **topic clarity**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why the Current Code Was Used After the Previous One:**\n",
    "\n",
    "1. **Improvement in Preprocessing:**\n",
    "   - Observing the prevalence of filler words in the previous output indicated a need for additional filtering. Adding a **custom stopword list** addressed this issue.\n",
    "\n",
    "2. **Enhancing Interpretability:**\n",
    "   - By focusing on meaningful words, the current approach provides **clearer and more actionable insights** about the themes in the documents.\n",
    "\n",
    "3. **Aligning Topics with Goals:**\n",
    "   - The improved topics can be better leveraged for tasks like document comparison, classification, or deeper semantic analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **Use in Subsequent Steps:**\n",
    "- The clearer topics can now serve as:\n",
    "  - **Features for clustering documents** into related groups.\n",
    "  - **A guide to refine further preprocessing** for different datasets.\n",
    "  - **Basis for thematic analysis** or comparisons between texts.\n",
    "\n",
    "the current code improves on the previous one by eliminating noise, generating cleaner topics, and enabling more focused analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['ai', 'things', 'probably', 'say', 'decision', 'kind', 'human', 'actually', 'basically', 'different']\n",
      "Topic 2:\n",
      "['think', 'data', 'lets', 'know', 'say', 'income', 'dont', 'course', 'really', 'want']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to read and preprocess docx content\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Custom stopwords list to remove common filler words\n",
    "custom_stopwords = set(['yeah', 'ok', 'umm', 'just', 'like', 'bit', 'maybe', 'right', 'thats'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in custom_stopwords])\n",
    "    return text\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  \n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation of the Code:**\n",
    "\n",
    "This version introduces an **advanced preprocessing step** by incorporating **Named Entity Recognition (NER)** using SpaCy. This enhances the clarity of topics extracted by the Latent Dirichlet Allocation (LDA) model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Features of the Code:**\n",
    "\n",
    "1. **Named Entity Recognition (NER) Filtering:**\n",
    "   - The code uses SpaCy's `en_core_web_sm` model to identify and remove named entities (e.g., names, places, dates, organizations) from the text.\n",
    "   - By excluding these entities, the focus shifts to general terms and patterns, reducing noise caused by specific names or details.\n",
    "\n",
    "2. **Stopword Customization:**\n",
    "   - The code expands the custom stopword list to include more filler words such as `'say', 'lets', 'think', 'know', 'course', 'really', 'probably'`.\n",
    "   - This refinement ensures that the resulting topics are not dominated by irrelevant or generic terms.\n",
    "\n",
    "3. **Enhanced Preprocessing Pipeline:**\n",
    "   - Converts text to lowercase.\n",
    "   - Removes digits and punctuation.\n",
    "   - Filters out named entities and stopwords.\n",
    "\n",
    "4. **Topic Modeling:**\n",
    "   - The vectorization step (`CountVectorizer`) converts the cleaned text into a **document-term matrix**.\n",
    "   - **Latent Dirichlet Allocation (LDA)** is applied to extract two topics (`n_components=2`).\n",
    "   - The top 10 terms for each topic are displayed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Previous Codes:**\n",
    "\n",
    "1. **Addition of NER:**\n",
    "   - In previous versions, named entities (like \"AI,\" \"human,\" \"decision\") were included in the topic extraction process. While these entities might occasionally be relevant, their overrepresentation could obscure broader patterns.\n",
    "   - The current code removes such entities to focus on more general and **conceptual terms**.\n",
    "\n",
    "2. **Improved Topic Clarity:**\n",
    "   - By removing named entities, the extracted topics emphasize broader patterns instead of being skewed by document-specific details.\n",
    "\n",
    "3. **Expanded Stopwords List:**\n",
    "   - The expanded stopword list further reduces noise and highlights meaningful words.\n",
    "\n",
    "---\n",
    "\n",
    "## **Expected Results:**\n",
    "\n",
    "- **Topics from Previous Code:**\n",
    "  - **Topic 1:** ['ai', 'things', 'probably', 'say', 'decision', 'kind', 'human', 'actually', 'basically', 'different']\n",
    "  - **Topic 2:** ['think', 'data', 'lets', 'know', 'say', 'income', 'dont', 'course', 'really', 'want']\n",
    "\n",
    "- **Topics from Current Code:**\n",
    "  - The new topics are expected to:\n",
    "    - Exclude specific names and entities.\n",
    "    - Highlight key themes or terms related to the general context of the documents.\n",
    "    - Example: [\"decision-making,\" \"data analysis,\" \"technological impact\"].\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of This Code for Future Steps:**\n",
    "\n",
    "1. **Generalization:**\n",
    "   - Topics derived from this process are likely to generalize better across different datasets since named entities and filler words are removed.\n",
    "\n",
    "2. **Suitability for Downstream Tasks:**\n",
    "   - The output can be used for:\n",
    "     - **Document classification or clustering.**\n",
    "     - **Thematic comparison** across files.\n",
    "     - **Keyword extraction** for summarization.\n",
    "\n",
    "3. **Focused Analysis:**\n",
    "   - By eliminating unnecessary noise, this method lays a stronger foundation for deeper text analysis or comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Was This Update Introduced?**\n",
    "\n",
    "- To address **entity-specific noise** in the previous results.\n",
    "- To enable broader **generalization** and clearer topics by eliminating unnecessary details.\n",
    "- To prepare the data for future analyses that require a **higher-level understanding** of document themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['data', 'nt', 'income', 'things', 'people', 'want', 'time', 'look', 'lot', 'different']\n",
      "Topic 2:\n",
      "['ai', 'decision', 'based', 'aspects', 'language', 'database', 'autonomous', 'driving', 'intelligence', 'ultimately']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "\n",
    "# Load Spacy model for NER (Named Entity Recognition)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to read and preprocess docx content\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Custom stopwords list to remove common filler words\n",
    "custom_stopwords = set(['yeah', 'ok', 'umm', 'just', 'like', 'bit', 'maybe', 'right', 'thats', 'say', 'lets', 'think', 'know', 'course', 'really', 'probably'])\n",
    "\n",
    "# Function to preprocess text and remove named entities (NER)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in custom_stopwords])\n",
    "\n",
    "    # Use Spacy to remove named entities\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([token.text for token in doc if not token.ent_type_])  # Remove named entities\n",
    "    return text\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  \n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    wn_pos_tags = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = []\n",
    "    for token, pos in pos_tags:\n",
    "        pos = wn_pos_tags.get(pos[0].upper(), 'n')  # If the POS tag is not recognized, default to noun (n)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing in a single code\n",
    "#import nltk\n",
    "#nltk.download('words')\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "def text_cleaner(contents):\n",
    "    contents = \" \".join(filter(lambda x: x[0]!= '@' , contents.split())) # removes any word which starts with @.\n",
    "    contents = re.sub('[^0-9a-zA-Z]', ' ', contents) # substitute all non-alphabetic characters in the string with spaces.\n",
    "    contents = contents.lower() # lowercasing\n",
    "    contents = re.sub(' +', ' ', contents).strip() # replacing more than one space with 1 space and remove any leading and trailing space.\n",
    "    contents = nltk.wordpunct_tokenize(contents)\n",
    "    contents = [word for word in contents if not word.isdigit()]\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]\n",
    "    contents = [word for word in contents if not word in set(stopwords.words('english'))] # Keeping only the non-stop words in the string\n",
    "    contents = lemmatize(contents)\n",
    "    contents = \" \".join(contents)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code introduces a **comprehensive text preprocessing pipeline** which performs various text cleaning and preparation steps before further analysis. Here’s a detailed breakdown of what this code does:\n",
    "\n",
    "## **Key Features of the Code:**\n",
    "\n",
    "1. **Text Cleaning with `text_cleaner` Function:**\n",
    "   - **Remove `@` symbols:** Any word starting with `@` is removed, likely to exclude social media mentions.\n",
    "   - **Substitute non-alphabetic characters:** It replaces any non-alphabetic characters (like punctuation and special characters) with spaces.\n",
    "   - **Lowercasing:** All text is converted to lowercase to standardize it.\n",
    "   - **Remove extra spaces:** Extra spaces are collapsed into a single space, and leading/trailing spaces are removed.\n",
    "   - **Tokenization:** It breaks the text into individual words (tokens).\n",
    "   - **Remove digits and non-alphabetic words:** Any digits or non-alphabetic words are removed.\n",
    "   - **Remove stopwords:** The list of common English stopwords (like \"the\", \"and\", \"is\") is excluded from the text.\n",
    "   - **Lemmatization:** Words are lemmatized (reduced to their base form), considering their part-of-speech (POS).\n",
    "   \n",
    "2. **Lemmatization with POS Tagging:**\n",
    "   - Each word is tagged with its **Part of Speech (POS)**, and based on the POS, the correct lemmatization process is applied.\n",
    "   - Words are lemmatized into their root form (e.g., \"running\" becomes \"run\", \"better\" becomes \"good\").\n",
    "\n",
    "3. **Reading .docx Files:**\n",
    "   - **`read_docx` function** reads the content of `.docx` files, extracting the text from all paragraphs and concatenating them.\n",
    "\n",
    "4. **Iterating through Files in a Directory:**\n",
    "   - The script iterates through all files in a given directory (`directory` variable), and for each `.docx` file, it reads and preprocesses the text.\n",
    "   - The cleaned text is then stored in a list (`texts`), and filenames are stored in `filenames`.\n",
    "\n",
    "5. **Displaying Cleaned Text:**\n",
    "   - The cleaned text for each file is printed for verification, by this way we can visually check how the text has been processed.\n",
    "\n",
    "## **Function Breakdown:**\n",
    "\n",
    "- **`text_cleaner` function** performs the core preprocessing:\n",
    "  - Removes words starting with `@`.\n",
    "  - Strips out non-alphabetical characters.\n",
    "  - Tokenizes, removes digits, filters valid words, and removes stopwords.\n",
    "  - Lemmatizes the remaining words and reassembles the cleaned text.\n",
    "\n",
    "- **`lemmatize` function** uses POS tagging to apply the correct lemmatization based on the word’s part of speech (noun, verb, adjective, etc.).\n",
    "\n",
    "## **Expected Output:**\n",
    "\n",
    "For each `.docx` file in the specified directory, the program will output the cleaned text. The cleaned text will have:\n",
    "- No special characters, digits, or stopwords.\n",
    "- All words lemmatized to their base forms (e.g., \"running\" becomes \"run\").\n",
    "- Words such as \"better\" would be lemmatized to \"good.\"\n",
    "\n",
    "Here’s an example of what the output might look like:\n",
    "\n",
    "```\n",
    "Cleaned text from document1.docx:\n",
    "this study focus on analysis of data from various sensor to predict future trends in health care.\n",
    "----------------------------------------\n",
    "Cleaned text from document2.docx:\n",
    "ai technology is becoming increasingly important in decision making across multiple industries.\n",
    "----------------------------------------\n",
    "```\n",
    "\n",
    "## **Why This Code is Useful:**\n",
    "\n",
    "- **Data Preprocessing for NLP Models:** The cleaned text is ready for further natural language processing tasks such as topic modeling, sentiment analysis, or text classification.\n",
    "- **Consistent Format:** By removing stopwords, digits, special characters, and applying lemmatization, the text becomes standardized, reducing noise and improving the accuracy of downstream models.\n",
    "\n",
    "\n",
    "## **Potential Improvements/Modifications:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text from AI2.docx:\n",
      "little bit somewhere intermediate great feel confident might learn ai good know otherwise hope positive background knowledge already available also see room learning success hope help aiming learning quite lot ai expect well gaining solid foundation underlying sure cover extent let try let try see definitely talk ethical think pretty much find small machine learning speech recognition well forth couple fix interested rather broad spectrum technologically metallurgic work well application various like image education prefer final project honest yet made well actually discus little bit think survey thanks lot go bed thinking request final project last week last week last year traditional seminar style way student basically gave presentation collected material particular also hello problem come collected also team break material basically develop last session basically nice predominantly yeah looking ai quite often ai concept always point explaining briefly ai applied health automotive industry different education provided nice overview also work actually get ai particularly also model like thought oh perfect time want replicate exactly thing similar thing thought actually adopted little bit run adaptation found relevant would actually something well adaptation last year similar last year nice actually team material course session also time would like result presentation report write also would like something four day one would said something tangible probably tangible would like yes kind code product applied gallery one one article would go tea church think something always two one aspect basically project work together scope also learn lot working together slight tendency say three used good number time experience already tendency maybe one person working le rest kind might three pretty efficient pretty bad would nice maybe agree common umbrella fee probably necessary something thought well might nice something rather also exchange also class little bit one might complementary something would say interest topic sense little bit plan general idea yeah think well reduce time next next three maybe little bit build would leave form going assign somehow leave yeah come proposal maybe might write baby class find common probably let let talk differently one thing thinking aiming pushing direction one thing thinking actually really situation also may point view university like construct university actually could use ai improve yeah various various lot somewhere purely administrative education classroom education also recruitment went also recruitment process know probably kind yeah maybe would potentially ai semester course could plenty one somehow common common theme university manage step basically include ai framework couple top could easily designed question also would whether whether yeah need need slash possibility actually align also maybe registrar service office say would like something pretty much recruitment team say oh going maybe even least final solution probably think would typically beyond call kind prototype say something could simple check example could answer student train information whatnot trying lose typically would like see project well university also kind scientific approach getting prototype running would also relevant kind systematic evaluation stick example designing student recruitment keep touch potential kind evaluation whether prototype actually successful satisfying need also evaluation validation point somewhere really deal laid pretty much beginning say take well typical example would say nowadays ai lot many say available fundamental fashion large language big trained going train large language model race expensive would like would like adapt large language model maybe general purpose tool specific would like fine tune data really specific theme talking also wan na get gift student answer student yeah undergraduate computer science want give student answer valid maybe would like specific answer particular university place therefore read eat adaptation correctness answer certainly aspect use evaluation might well maybe engaging also conversation easy yeah fast might one evaluate could think think would reasonable project topic would say one hand clear cut idea kind ai method tool use context applied evaluate produced probably done certain prototype level fine limited amount time semester basically built immediately also issue question actually get background information oh additional point probably might able address also might able address know ultimately system would overall infrastructure university would side actually run another question also typical ai implementation usually setting assistant scratch well infrastructure various running decide point oh yeah add see kind system maybe available well yeah information available require kind need actually extract handbook information example something yeah would sign also project say something cover know need link need established want roll final product side find project could look like would hope certainly schedule would typical thing expect maybe end submit short proposal would like maybe representation plan kind project progress presentation towards probably end final project presentation see another step quest audience happy clear good good talk maybe find name might come talk next time good artificial intelligence honest number growth maybe resolve answer definition artificial intelligence would define crucial data data something else algorithm algorithmic said learning something action like actually act human acting human proper thick meeting something like like accurate yeah accurate probably additional demand put top probably facet intelligence say whatever intelligent also accurate certain degree yeah also connection acting like human intelligence somehow something would say act intelligently another another debate start probably moment aiming something action human decision taking human usually intelligent basis expect pretty much level curiosity also level probably fairness ethical expect human expect machine basically probably see dilemma particularly come autonomous driving probably still one prime ai usage nowadays autonomous driving algorithmic action human behaviour also expectation autonomous car must drive least safe human actually yeah might even demanding respect accept make accept happen whenever one accident autonomous car people really put question whether ai working whether autonomous driving ever take take might much much higher question liability responsible damage well behaviour system quite clear typically expect human viable accident driving car person driving liable autonomous car always still ongoing discussion going liable liability basically programmer point either mistake decide fatal accident potentially need algorithmic decision hit wall driver hit group one well foresee might occur design kind solution flip random coin randomly car either wall group complicated ethnic group elderly people whether difference getting one person multiple decide something ethical dilemma liability also right decide might also culturally different better appropriate kill kill child take account child die take account elderly person whether whether might different different towards large kind liability necessarily question ai per se come addition matter yeah matter accuracy end also expect multiple basically yeah know ai system say might produce least debatable let let put way might gone definition ai clearly certain degree aspect personally quite often used term automatic decision making ai although decision making might already restrictive ultimately transform pretty much every action also decision probably true even write sentence generate also bit ai generate text boil say say oh always decision next word sentence district decision making probably people might say decision making restrictive limited potentially action human like action decision making decision making always little bit yeah rational structured probably lived human least quite often also go next slide see found page simulation human intelligence think act like various cognitive abstract thing learning reasoning problem perception language comprehension definition also cognitive technology actually able problem probably one thing always foreground problem decision making rather another certain degree perception particularly language speech comprehension yeah actually action understanding part historical still might probably complicated reason really really give argument certain done way done certain done way something also lot development probably still need done yeah since 1950s slide yeah different basis development f ai historic go back basically ai went number different phase also time also different different focal problem really beginning idea construct artificial neural really mimic neural system brain human brain message slash machine way rather simple artificial neural binary artificial neural could say yes like general structure neural laid ago take jury think really solve open data would like various difference yeah one hand technical side kind assistance language side general also going away binary logic yes fussy eight logic binary going basically face 1960s first rise ai many time conceptually great really put practical real time yeah power limited also good practical implementation nothing really could say great great application led certain decline ai winter basically much happening middle 80 basically time also name ai actually admit neural network back propagation prediction based network popular feasible take time headline data analytics data science much headline ai probably ago still ai popular even would say yeah data science probably shown sometime famous quote barry time chief economist said said next decade would sexy job already later people statistically later probably would call ai developer ai science fundamentally would say well fundamentally good question underlying conceptual three different ultimately dark age ai early ami pretty much use lot data structural technically built formal logic saying logical system would like based based logic would like develop system like human rule based system striking meant actually combination yeah exactly beginning middle 90 instead aiming finding rule system give idea basically always always say combination huge data corresponding power system somehow rightly together say crease success also increasing quality guess one hot look ai interesting thing like quite number request fully comprehensive clear moment business landscape much ai supportive variety one hand certainly nice success data available technology available addition quite number air force short always driver labour force scarce think replace driving one try somehow group different clearly lot ai might ask question difference data analysis system ai system example data analysis pretty much difference data analysis system ai system might small area classically credit apply loan quite long longer period time already used credit scoring system internally information whether back typically ask new apply new loan certain regarding whether house whether much money banking like married number apparently past positive back paying back many straightforwardly interpretable like yeah someone married family another le likely simply disappear easily possible higher higher chance paying back people involved another sense responsibility maybe well obvious say principle information already informed time taking account human making decision point basically automat scoring system still end human took decision said score bar bar also already ice system would ultimately also take decision basically also ethic well actually already previous time become prominent ai system human human decision taker bank also fa give loan person giving loan person maybe also reason people actually happier point four yeah kind compute core person bar like braiding usually two different sometimes grading one thing go read report thesis thesis thesis admit often anyways first thing read thesis overall impression say oh excellent thesis good one good one good still pas overall idea grade going usually also grid put say aspect aspect maybe time two luckily align borderline say oh good thesis good thesis somewhere always helpful well easier make decision say oh apparently scoring came something despite fact somehow torn example decision easier understand also human typically number go along decision decision easier corresponding know hand automatic system far guess exactly also point maybe question difference artificial intelligence human intelligence human yeah somehow think might even take account many might might able know student know kind received might say oh maybe harsh great high slow somehow modify decision take kind reasoning guess take account probably beyond hope debate whether actually good additional taken account additional taken account taken account certainly also potential negative bias also potential positive bias kind reasoning probably rather complicated machine actually pursue woman might one separation human intelligence still little bit ahead human able ponder reflect certain maybe really foresee various might happen might better maybe hopefully better system probably also intelligence human least might capable maybe still majority capable well might one clearly one broad ai close come customer service probably application also large language success speech recognition large language last really field custom service extremely extremely extreme yeah certain degree well somehow based data specific field basically last language opportunity understand text create text generate text communicate based communication machine rather predictive analytics closely related top point state analysis clearly something also long time probably also former time well data analysis prediction probably made done specific time made quarter maybe predict future basically every second automatically done time continuously respect also aspect ultimately personalization probably also much made depending large language also depending much data inside well say based user behaviour people done based maybe also social medium information people somehow expressed would like kind different together somehow much easier product around user probably aspect different somehow come together look course course ai business society let also look little bit business societal information business aspect clearly eye much also field kind largely come mixed decision making yeah kind treatment done also come try rather treatment plan patient particular better treatment example nowadays based number really many really see cord right whether decision making somehow still think also sector reluctant say oh decision taken eye system would always say yes yeah underpinning support ai system fine decision doctor well patient would like forced one treatment system something like always ultimate decision medical professional whether want also decision patient whether agree treatment least long one sometimes also feeling actually ai used really content necessary say oh opposed decision making system education also yeah throwing area probably indeed decision making probably difference education education prob nobody people worry decision taken system kind force material ai system lot information discipline field included basically ai system based individual performance test questionnaire whatnot next next session going get next information session kind kind text getting kind presentation get multitude covered maybe topic different format one presentation one presentation another professor last one maybe video comic rather entertaining way might different covering content different people individually might react differently different presentation might learn done learn seriously expressed people different would say long ultimately successful nobody would would care whether decision got one watch nobody would care education automatic system whether get one drug people say oh want human loop ultimately decision probably say let work work successful happy though kind relevancy probably true probably impact whatever treatment get anyways never know life based context probably decision taken machine weigh usually people weigh le health care transportation yeah yes one visible autonomous getting calmer moment much five ago even three ago guess last two rather rather silent work 1st legal liable fully autonomous driving also yeah think mixture work work cop also reason starting basically lot autonomous driving going happen near future already happening brown transportation airport something either easily ultimate driving restricted space basically limited access clear enter unlimited number potential traffic situation little street lift kind well airport basically people anyways move gate bus bus aeroplane think maybe change runway believe rather conspiracy real real traffic actually probably really easy easier city traffic german city probably much easier row traffic already chaos grown used ultimately criminology society security probably one big societal well lot automatic rain whether whether lot come come also clearly repeat public fear pretty delicious issue clearly ai also lot dangerous guess point make break little bit confused said say regarding time continue second round hope agree despite written differently composite moment sort already announce next week away conference next week class anyway two hope sorted correctly ideally would start vote let go break rejoin afterwards\n",
      "----------------------------------------\n",
      "Cleaned text from Vir1.docx:\n",
      "mine last week seminar prepare quite stressful semester new even confusion getting getting registered something like least good found already registration also class today start last year basically right visualization communication visual communication data story telling let share know already screen provided another organization found problem le technical reason invite blocked try invite today get get get hopefully everything worse work idea get invitation today little massive massive probably one sandbox positive also project project similar data science lab course also provide repository one information go page already click syllabus site think yeah hope many already rough schedule precise maybe make precise internal internal rep course organization repository yeah general structure already maybe adjust bit depending happening come bit later hopefully toward end today session find think also link page think also provided encompass net also get link also also team hybrid one guess maybe arrive try provide hybrid setup make clear course designed person presence session think four something like like individual bit similar also course lot main assessment project work may make sense direct interaction individually project work think hybrid participation work hope end sitting alone entertaining screen yeah also session really work think example today session ask yeah discus neighbor briefly discus bit together principle may organizable much harder easy spot yeah need try offer hybrid setup way work yeah think possible best setup let show today get topic end like yeah today visual communication data storytelling let go right contact yeah somewhat lot communication usually think verbal communication mostly mind reality lot communication visual provide something visually table whatever even 3d provide build something u communicate also part communication think communication sending information perceive maybe perceive differently maybe perceive maybe provide complicated way complicated decode generally come come come place provide visual say data science work yeah go super deep like general general introduction saying people remember much hear much see really sure come yeah think striking lot remember better yeah type visual communication start probably also type visual communication leaving type also many factory communication yeah leaving leaving factory smell lot visual sense sense u would say way think visual visual visual perception sense second really full knowledge question really second second vision another smell touch think think hearing get information course smelling touching also important special touch far reaching right touch wall side maybe hear something think something perceive information something would say structural difference vision hearing work somehow far reaching hear someone something far away see something far away structural difference look store information record data visual data sound data structural difference yeah something yet anyone idea bit huh yeah converted something also audio format think dimensionality see picture many hear noise dimension one sense right also directed vision look sense hear hear backward first really possible technically possible make sense right much yeah linear directed vision watch around want also talk linear way go step step everyone follow order saw visual everybody look still course structure visual perception maybe people look thing first something like want look bit learn bit think also nice topic yeah practice better data work data visualization work create figure create figure audience look first type anything slide need need discus visual communication early even cave let say ago somehow visual communication like like like like like many show alphabet really visual communication alphabet textual communication let say speech also write becomes also visual communication type writing writing become also crucial go deep people really typography yeah think also interesting interesting interesting field let go deep statistical visualization graphic design nowadays even video even advanced modern form visual communication course mainly focus statistical statistical course somehow yeah surrounding general visual communication good much general go visual communication first question read scientific paper maybe 70 usually read want get information paper read first probably title right huh could give something yeah right right yeah absolutely yeah scientific paper abstract often executive summary rest step step linear way read complete know ah read conclusion go back go back beginning see stopped think conclusion interesting enough yeah yeah also often course skim right try sense maybe show think fairly similar think also bad like sense thing skim look try perceive message whole article maybe get already idea telling try read figure caption therefore often informative lot dialogue article sometimes caption really long dense information often like lot get detail especially know already bit already want background information think caption much red text middle paper introduction start normal reading maybe conclusion emma saying sense really even read text often sometimes lot sense even always case use visual communication say jump back forth also reading perceiving text also yeah super essential think many spend much time craft article much lot time much text would say least course also writing text lot time also need need done care go go idea want compelling bring large part message around write text somehow bit even around quite always formal word count course want lot text generic want information condensed graphic cool much value think also generic text word count really looking although somehow need formally somehow look slide read everything slide work way read first read read like nine read first like one yeah skip something anyway main headline fully precise way use huh yeah see fully precise go top bottom right go way sense first reading subtitle good idea sometimes interesting main generic anyway read main point order order fully precise order control fully graphic something like like design fully control first control emphasize somehow want go visual communication let remember get back time make break right yeah yeah talk neighbor like discus bit something something short 3d data interactive graphic even page data idea next two hope give framework also provide discus discus together next two class side go bit already something something mind think remember next may want look let look bit detail sure best choice read see tell need yeah see also ready go see slide show view edging linearly cover actor mean yes tier bluish edge time deer example right see another staging right yes technology getting connection yeah think really think pattern continued found found twitter think good sign even bit like like red cut road like maximum point yeah late turnover maybe get go deepen depth point really way statistical graphic also mean memorable say nowadays come come back come back come back minute fancy type visual communication 3d used everything 2d maybe produced 3d data visualization probably found quite hard comprehend typically advise use 3d paper 3d really 3d found yeah day ago exhibition park anyone knew park museum nowadays spot rural area code working break enigma code maybe saw movie imitation game nice story alan founder one founder breaking big part yeah big big mix mathematics computer science linguistics everything also museum also also exhibition art data think see actually know exactly orange white checked probably maybe inner town would think buy one probably right quite much much new york think city large much yeah think site probably think even bit lower although still quite large sense living premium city already support much smaller hong thank think also video much even smaller video watch another video similar topic anyone know video even better even know would anyway shown prominent video think take another data visualization course necessarily video story telling u quite common one yeah know exactly yeah quite quite quite important data visualization video unfortunately already dead let let listen visualization right heart work teach global health know data enough show way people enjoy understand going try something never done animating data real space bit technical assistance group go first next health life expectancy access wealth income per person four six rich healthy going show world ago come brown red middle east green south blue yellow size country bubble show size population pretty crowded sick poor life expectancy slightly better much start world industrial revolution elsewhere move away rest stuck eventually western get slow show impact first world war flu epidemic catastrophe speed 1920s 1930s spite great depression western forge towards greater wealth health japan try follow stay second world war stop bit look world great year war topped medal table winter born difference world ever united front japan catching brazil way behind getting little oil still short china still six look happen go lifetime former independence finally get 1970s catch western stuck civil war hit see world today date statistic people today live middle huge difference time best worst also huge within show country split take china split go shanghai wealth health today pouring line apply show like split rural like yet despite enormous today seen remarkable progress huge historical gap west rest become entirely new converging world see clear trend future aid trade green technology peace fully possible everyone make healthy wealthy corner well seen last story shown beyond involved plotting pretty neat huh question mark 18th century china five done probably course quite unsure really think try estimate probably various way probably already time course also inflation whatever measure unify 18th think lot work involved take probably maybe life span data bit easier get income data also probably quite hard come come yeah yeah right think per deep economics think many many say per basically transferable average income huh per year per year yeah per year yeah probably per year yeah typically per year statistic yeah typically yeah know average income data let say information way easy digest simple graph think purpose purpose main message say yeah one face message mainly good better often think think message somehow longer perspective let look video think another guy get back also seeing right minute another guy made video might also seen like one income along axis life expectancy along axis different color size mirror number might seen last two ever increasing earnings buy seen true say something important world today however used propaganda justify state hail western capitalism ideal society extreme like completely take look instance really world like look u neatly small little bubble small mean income reality reality national income unequally distributed among bottom half population average income per year much lower national average half u income roughly four national average income increase understand look entire distribution top zoom bit among top average income million listen carefully since share national income going top almost twice large share going bottom half population whatever huge impact national average last top income half population poor national misleading like affect long people live woman belonging top average life expectancy whereas man belonging bottom average life expectancy mind gap yes gap u exceptional respect zoom come relatively egalitarian society take tube well educated road le educated voice record price take arrive life expectancy improve year bit longer watch full want time video yeah problem average right everywhere whenever people talking average use side story want say right yeah every dot much one value two dimension think good good point said reality different course bit question really reality course average also reality lot main point average know inequality say right way also put information yeah think clever data visualization probably possible put put everything course yeah showing inequality way looking data visualization insulin inside average maximum minimum right something nowadays possible still lot work something like interactive right looking one thing showing thereby showing giving sense variation also maybe even variation large saying mostly every country large think true large wealth people income le lot lot income also anyway usually men also also different also another another question two video think le data maybe guy let check name maybe slightly data set compare roughly spot bubble looking differently maybe briefly discus come spotted precise better please access scale color size got yeah think much better go right one easier yeah absolutely yeah yeah income zero got yeah cool color yeah also come yeah white board distance like access 100k small distance two something different entire scale one different make yeah right mean really right privacy yes conference different know well message yeah yeah yeah exactly gon na actually maybe realize yeah among think problem yeah think close probably blessing like somehow least mean fast would impress right like stuff yeah awesome let right yes yes yes time almost redundant right human right dollar guess could also know right mean always weakness let let collect together difference spot back yeah talking like condensed honest really hard tell put income continent country mean yeah one clearly see press blue like easily see one yeah difference technically say looking data yeah sorry yeah left income access go zero 40k right go zero k meaning left everything put right one everything close entire scale latter right yeah sense income progressively higher see like dot 90k income yeah yeah everyone see one freezing dead think would better become anything true would see size better think summarize table soon yeah two two think essential part missing scale got already different scale think stayed data going bit one reason maybe different counter size background first one quite bright right yeah ground quite see clearly also access think recording modern much good quality something meant color visual see different measure see difference yeah mean colour different different color mean basic say color continent continent continent guess actually got yeah yeah yeah yeah think color exactly hard hard compare example maybe remember talking lot would say give one one yeah probably justice probably green yellow rest think maybe match although really see orange orange probably blue red yellow green maybe hard easy spot size size everything right size dot mean population right get size besides always question match size radius dot area dot radius area actually perfectly sure get difference area radius population country let say billion china roughly think time anyway matching matching area would would would difference let assume let assume radius china china radius fund previous four time area wait second million thousand million let say probably like like radius roughly radius would happen go change making proportional area proportional radius much smaller much much smaller china think proportionally somehow right china would matching maybe fully scale point better better measure better visual area population would change correct proportional representation percentage radius would change german bubble bit china bit smaller go small time time smaller one go area area go area look like think maybe make precise good visual reconsider let let go let go china may work would sorry hundred hundred percent right le scale matching square side hundred square side proportional area right much proportional go proportional probably like like like something like right think think better radius area size match something like total population useful area mean area intuitive would say bit harder yeah right construct kind yeah yeah right think reconsider remember important topic think proportionality aspect area much better typically choice yeah think fit know fit go go one dimensional measure large get way large say square everything right go area square everything side whole thing area middle middle question course much right difference much difference much probably best way visualize use use size two dimensional object go area mean something square root typically match want want make area dot proportionate actual number take square root size think remember probably use soon even something like size area visual typically use use like anyway good idea look mass make clear may bit confused maybe forgotten formula right like grab pie always square square easier see square root immediately essentially take square root proportional check already break think reconsider point check fluffy look data set behind data set behind number number population yeah else life span yeah per let see people income also country continent think come mean yeah right country say yeah also right everything right let first look ross ling maybe remember grammar graphic basis package package always start data match match athletics look population maybe remember aesthetic maybe come find population size yeah size object size typically size matching continent color country last time right think visual visual standard think year label label another thing static plot actually plot one year right really really variation variation dimension number white number time except year year video time point something put directly make movie also always think dimension time course natural one also time time principle could also say let go video time weird thing often done label course correct whole see snapshot data static visual filter data variable variation come time break maybe also check population right size different access two rest right x color slightly different axis time something like would ethic would could also something scale scale several data already limit limit yes thetic probably sure scale area really double checked guess like compare let say china u think size u four time smaller three time smaller roughly think area probably well right area report le three time area yellow radius yeah radius two time therefore pretty sure area also correct choice already point le zero sorry yeah good yeah zero roughly income zero need left though yeah yeah right yeah right roughly yeah probably bit yeah something beyond right also one important difference yet scale idea reconsider fact artificially extend could really want full range difference need need cut cut get would think still variation would look different variation right white income also different scale income data yeah like one linear one like like yeah yeah yeah least first let take logarithmic right important first recognize linear right go time let go distance even time logarithmic scale sense time yeah plus k plus 20k plus 20k time typical logarithmic scale logarithmic scale still original original value often quite useful data visualization yeah probably scale logarithmic sort cold nice still interpretable something quite important often data visualization make technically data exploration often take logarithm plot look easy interpret background knowledge making public lot sense like course also importance say people time yeah dude time le time le much much yeah absolute lock scale linear scale could style transform transform data let take look data let say cloud like need like bring graph somehow flip flip axis mirror like go yeah also feel x mirror along axis also switch switch somehow right bring bring bring bit also say right way data thing slight different color scale think know essential make data quite quite different graphic course even change graphic play first entry point conceptually thinking present present certain way let discus bit type short break great oh yeah yeah thing like coffee line yeah get something yep sorry yeah registration tried registrar problem least yeah need two data science register miss requirement yeah wine could sense great right think like read read unfulfilled since know past already create trust algebra yeah want probably right yeah see series confirm like error side also yeah also push german word past missing yeah also also great yeah see yeah yeah communication unfortunately possible yeah go office see yeah send see see maybe back confirm know know anything solve think everything done even say like weird error think done know time come add team probably right yeah yeah yeah maybe probably soon way also got already information contract think worked already old call think mine new think hope contact within week hopefully could start 16th anyway thing student bit think bit think able really start work student coffee like slow see would work think even feasible one hope contact start week yeah certainly want step guess thing bit cost center central fund big course made request assign bit later hope guess week maybe put ask guess maybe hope week come short notice course imagine probably often like maybe time year would good yeah lot think lot peter process typically try make exactly state 16th starting hope work think come information weekend fast ready think work let see contract conflict yet course yeah think still still normal quite tight schedule see use believing two year two next posting contact maybe even could get everything still still yeah also somehow still missing yet also new site think basically think bit maybe something moment like fine find work silly well already work even good one already late right even yeah exactly yeah think continue anyway next story probably adult go super deep graphic skip back come next topic otherwise need watch video still running yeah build think got bit also sense different political message say right one see progress world development also world development data say sense german course right know german book something yeah like right huh right right data yeah link left think political yeah political politically right yeah like yeah let think think idea look data different side see different depending present mean know end must home see see see want want see right yeah see want see point yeah way short fun fact interview author said gen z people got title wrong starting data german word date someone could also read left wing people dating right wing people data yeah one graphic book elephant graph yeah elephant graph see see percentile global income distribution top bottom income distribution people probably side even poor people rich people probably also except super rich scale maybe top world income something state change increase percentage think percentage change real income income top five top middle say population change percentage certain say gon na say think message graph increase income almost everyone part among poor world among middle poor among middle say middle income middle class upper middle class really really upper class message graph highest increase income like supply people course really yeah one second think grease u site yeah actually case right one something like upper upper middle class world really super rich also also super poor increase lower lower middle class fully poor still poor highest increase think necessarily left right think type people could use political political message say message could propagate honest message think middle class people informed different way really income right section one talking people middle yeah yeah one yeah one great maybe baby baby somebody go purpose political policy might really affect right site lift right yeah could yeah idea political message tension majority say income increasing ignore like let phrase bit concrete want political message suffering would would given given given graph whatever political message want want say one though make elect something would message people range right perspective increase income last year people poor people much increase income suffering could negative political message trying motivate people stand sort saying upper middle class losing sense could message course also message also suffering anyway super poor really really suffering people course question want address people say suffering phrase let take super rich lot let take need support much increase right could story operation want decrease difference yeah try like grab attention population say going make income higher yeah rich like yeah right support trivial right access percentage income class time know absolute value know absolute know bottom top five already hard rest income income increase percentage increase would say last gain last people could political message people come late confirm even break possible get coffee get actually thought somebody next would go deep bit sight topic similar see similar data different political another example complicated graph elephant graph mainly saying let see yeah think graphic often used vestal u middle class send suffering world rest little class see middle class middle class much income data politically used way yeah yeah course somewhere let look another version graph one black one elephant people see upward trunk yeah bit one famous hockey stick also used lot climate graphic temperature increase hockey stick going super much red really red black red idea easy grasp income upper middle middle middle income difference axis red one absolute gain much see different picture communicate black one yeah belong highest income see crazy increase yeah lot absolute people may percentage let say absolute yeah right saying yeah black black graph interesting message sense people lower middle class middle class middle class total world say total income wealth still still tiny absolute even people total increase even increase small increase percentage wise nominal even lower lower lower class get point really someone lying graphic issue lying statistic people say often anyway lying thing look data certain way necessarily wrong interesting information complete picture always good different looking data one question know graph graph know graph data infer graphic technically know think quite backward probably behind guess think information data say infer way typical typical could learn ask ask data think think black line know change axis plus know lower people know lower essentially one infer know know percent even know income distribution know bottom know much also know much guess course probably school distribution probably little absolute value average income group average income group every income group probably also logarithmic scale probably multiplying million million going always looking percent often quite interesting way also quite data wrong room know let see black black relative income growth meaning percentage growth guy hit percentage change look right look right yeah another topic also come often data visualization task axis one axis absolute change one axis percentage change typically advisable may interesting compare certain normally quite dangerous particular meaning point magnitude arbitrary scale scale ax right therefore must course compare factor see large elephant absolute relative increase income see absolute little compare two read properly quantitatively look red black basically yeah black line used say western middle class much last look red line still see poor people still quite cool gain still much u natural percentile two graphic left wing comically left wing data politically data data different way truth complete good look let see time also talk bit course maybe still time let let discus second point bit without maybe let assume data data five relative absolute income relative absolute income change relative income change let assume raw data raw data would data set really people people world five five billion year let say billion year assume data could make red black graph need process course data bit problem income compute compute question data data transformation need yeah several one mind come one need exercise course right get get need mean income growth need divide income year n income year n get income growth right let say yeah get income change yeah need divide income one think better yeah think broke right yeah ideal data format right yeah want new data data form thank need go transform way one yeah hear compute percentage change yeah use pivot type right right need individual much course people dead people new labor force probably fully work need right step fine need one step right want also want income change individual change present access yeah right oh good principle good idea one thing need change first need compare first compute compute yeah year need condense data set million convert something like per cent tile let say five percent detailed want probably zero five percent typically want even even smaller top top top world work top whatever percent percentile something like income average median income group know compute change yeah dividing would say dividing yeah get get get percentage change divide change another relative change enough absolute change would something else right step right real quick two already already highly problem bit reduced bit hard conceptualize least need people become become whatever easy process several happening happening meanwhile also income distribution may lot maybe income become much unequal much equal meanwhile would absolute drastic see graph looking full context lot background knowledge course could also first look distribution income something like hidden still interesting see often think income often thing start present please yeah yeah think often happen somebody one lower percentile many make percentile one upper percentile also yeah really really know think typically normal normal trajectory people get bit overall life span become necessarily necessarily yeah yeah know actually much yeah also know saw also example would interested see differ right maybe distribution exist maybe know yeah also know exactly right let let move point sometimes graphic skill data scientist practice basically time graphic see data data could often quite hard work involved often certain message also interesting decode need data probably data really wrong best estimate probably quite hard still way way also also specific specific specific purpose yeah already conceptual work quite quite quite valuable tool able data scientist criterion come soon let look dashboard maybe already yeah seeing say population permit edge yeah like edge permit bye yeah see edge distribution men guess right last horizontal path people course right yeah much raw data say count people male million many female say see see two also two ax quite quite nice put two maybe small see access one zero alright year year course one minus minus edge course relevant information see people around first year around good compare course absolute easy compare add see almost almost bit parallel see live longer right see leave right side yeah yeah see especially older bigger could see look manual bit bigger clear see image complex yeah think produced mean red blue similar minus right basically happening whenever make radish whenever make number already think nice nice trick much anything general perception clue compare let let look look look also time remember time natural thing let start 50 year born eighty old born born going go let let see see every year born year see see increase right success pretty yeah western u think baby know exist world come minute already coming maybe born already know exactly see check cohort compare lower though somewhere come daughter something good actually projecting color seen mean making yeah yeah right also color change color slightly change future projection clearly see projection something know flaw projection real positive think really chance getting right yeah let discus minute know access graph maybe also put want want come back god full screen get short short input know interactive graphic many interactive graphic real time data coming business data dashboard quite commonly used quite common could also quite common job data side side also side reading making making making informed explaining something like want mention something relate also relate course main part today look basically certain point practical relevance nowadays many data quite quite common thing one question question graphic need hierarchy simple short go want perceive data structure data hierarchy first start basic go quite relevant today come minute another source visual communication call multiply page data think saw page outward data already visit find good typically data interactive democracy index select various well also time maybe also bubble rustling think much transition tradition positive guy two also want map world want make u overlook positive general often often look negative current classical success typically literacy rate around world childbirth see basically lifetime declined tremendously child age declined tremendously last basically almost another thing already go data storytelling let start let go data storytelling last part let check organization data story data story go bit beyond visual use course also something like main story point main point solve graphic twitter think side link story three tree core basically related current news think two ago broken four graphic confirmation woman turned looking new data still confirming theory upper limit point one visual core story current news say another meaning story story mean context context yeah fully functional data story provide context pause yeah right everything story come provide compliant yeah rang data typically make much meaning sort present visually way still meaning meaning like bit think also bit mismatch meeting story data basically great data story get mean happening know think think mean yeah yeah right almost yellow kind really match right yellow also green special green composed thing sense think need general point interesting course main point data basically nothing get meaning technical get much meeting get much meaning right decomposable anyway last group work think try get meaning phenomenon important different right thousand current almost current future projection propose yes bottom line bottom yeah give thinking age speak two world maybe also part origin also many people let say let see take group question quickly discus make story question take b question story two 1960s nineteen story area large dark red area story gap maybe two often gap going say story happening happening first step happening 2nd step maybe idea happening discus let say go got question know bit think really nice right yeah additional war also flu yeah yeah yeah yeah right bit related directly question unfortunately see birth well think important maybe help correcting let let know want know think see age course come certain year visible see main main main line move interesting got get yeah yeah think let see yeah want go thank young man right yeah pretty much like watching yes know gun right yeah know honest like get like know maximum age know movie yeah father yeah go yeah actually yeah go group group mark around group b around around two right would around like direction one generation cool lot find man course next generation right front runner think yeah god first one person know thank take nothing choose story mainly see see maybe someone thank much make talking right look like yeah like make yeah see eight close yeah could read suspect might found yeah actual story well roughly let let start saying group see see could listen please stunning booklet see doctor red population since world war two also analyze range zero five see shrinking point likely lot widow think le possibility finding match le region right one generation lot find man surplus since many men apparently felt war next generation gon na right basically seeing already projecting bit probably also true main point much much much first question typically born look currency old typically age group either equal even slightly mad much also yes quick question think making right basic question many much lesson right wait le men yeah nothing worse give go bar time much le yeah yeah mainly mainly mail missing typically typically time mail missing front war age woman work start yeah worst five missing missing yeah year year typical right know german war history better even yeah end time war even young sent people dying probably probably consequent missing missing see gap course get transferred upwards yeah come bit closer normal excess see also without without reason board want interesting question maybe already next next would want repeat go question two mean said generational effect yeah lot find man vice guess see drop childbirth course next generation yeah guess right actually access mail place many big unless excess le place right drastic gap starting maybe related yeah let let let ask let ask think explanation ready gap course gap think gap right later guy gap destination basically two go two something right two come like come men men join army like effect potential maybe lead say birth maybe yeah see second gap slightly bird rate like yeah yeah yeah simulation get yeah select book baby boom generation yeah yeah baby boom generation really really really clear saw best basically age tight gap narrow five essentially three something exactly said war time gone front even family wife whatever could produce produce front maybe also yeah maybe know contraception pill available time still could also contraception way time also availability extreme actually mother think produced front holiday guess ship mother telling father grandfather barely know came back front something daughter saying grandfather photograph board father photograph real person fine yes see generation one one gap gap gap second first world war right first blood board phenomenon four much front home front say yeah anti baby right know like year pill rock yeah drop yeah sexual revolution right interesting point want hurt data yet hurt inventor pill death get talk talk said yeah story bit pill japan drop le year without without large scale access build think japan well story go pillow late japan remember exactly also funny story let go next gap yeah boring year gap born gap yeah yeah yeah one yeah saw 70 also may brewer maybe east anyway also west yeah graph low birth rate yeah sure low birth like well yeah slightly increasing late mid year bit let say generation actually gap gap slightly one thing course yeah abortion also contraception fill also explaining lot decline another even simpler story education even simpler story typically typical age get let say average year first child even let say think left people later also bit le generational maybe story generational transition get expect get another gap later le typical age child around think even see even see gap bit washing right yeah yeah generation even another gap let pronounce even another gap let pronounce small increase think still still nothing sent watching exact exact get child course lot rain something final group open tell u e virus yeah like see kind see first looking everything school yeah actually really sure corona effect maybe large enough maybe kind really yeah small entire pandemic gon na maybe took year pandemic want burst think affect yeah year yeah 1st know like maybe le first people born pandemic know still alive yeah projection steady decline say pyramid standing top really portal said right yeah diction yeah yeah generally phenomenon yeah phenomenon happening almost development demographic transition first first child child death decline worse population somehow trilogy go solid around saw 70 think still nation population china way think still growing see already part like like many still like like like still like actually anthropologist generic pattern still real super hard scientific explanation generic fact happening place general rule behind also good well done think bit way data storytelling work real data relate historical explain story around happening really see let see time yeah left let come whole term data visualization got already used lot think already hope interested develop towards used lot basic skill analyze new data also essential communication yeah storytelling focus course much getting graphic getting graphic thinking change improve purpose communicating may guess go back data transformation go back data origin think data maybe transform somehow make certain point transform explain example logarithmic transformation really necessary think necessary show best show locked probably show actual kind another technical chance increase look graphic next week basic made maybe look data data science lab maybe found possible let see yeah point point another thing technical go really want therefore first step develop sense want visualize therefore also next two want look also hope bring bring send message next day around think bring similar maybe find interesting find world data short visual story say yeah think interesting also lead already want make data story project future maybe bit fast think show also bit g plotting working python think transferable even interface python use mud plot lip show good example common really visualization saw lip course mark probably great graphic also bit technology way thinking grammar graphic style aesthetic transforming scaling way speaking say also graphic like really made bound due basic thinking lot sense also think course principle exchange right also continuous color map put color could play see sense message term also lot break use ah another interesting point new taken book yeah yeah good provide book course organization repository access provided book found clear indication illegal get provided absolutely sure also know library idea read book course chapter chapter indicate syllabus chapter super strict also provide syllabus think close reading important example two say mainly optional skim interested course think read everything book least least blue many clothes think stuff generally generally interesting also also may valuable written bit business perspective know much business person think perspective book also valuable also valuable general data storytelling want make project bit businesslike fine want make project societal phenomenon societal aspect many similar rustling whatever also fine yeah lot lot flexibility start thinking want yeah book work maybe want also input data visualization data storytelling business think also good resource two prominent also also good stuff also quite practical learn maybe select tool go many around think time next look starting bar density box whatever really fully bubble bubble like rustling many different different good look bit around get inspired try think whenever data look could make sense try something experience even quite advanced technicality technical way still often idea try look good like start work thought could put another aesthetic put size work thought lot also communicating think lot trial error trying new would happy go try try discus class would happy provide yeah also course think python right yeah think way quickly least try bit also inspirational could right technical could look much time still bit something soap showcase final step data also much business world book storytelling data typical challenge said visualization green direct red indirect really sure actually different company time running year close deal axis along close deal goal day see sometimes day time le day difference direct indirect way could decompose decompose message business presentation may better way show show information waiting two let say sense eliminate clutter remove important focus attention focus something like like like question bit clutter could eliminate get yeah yeah day day decimal science remove axis one yeah plus go away yeah also entire year remove look sitting yeah caption would remove text top bus well one right also also go away think yeah number know yet maybe maybe keep yeah change many yeah way good time bit bit lighter restart blue maybe light blue gray something yeah good point distract still give opportunity check clearly sometimes important yeah always want close deal maximum day yeah right highlight maybe exceeding day right sure yeah actually right wrong try think better solution also different type visualization bar chart design right yeah think also found often easy compare actually want see trend green red compare also proposal put line indirect problem every data point dot dashed goal line something reference line direct problem idea maybe compare learn direct faster indirect think something particular june happening actually know know idea business case come also bit bit technical time come come later also something keep mind would never type clear story point course end still lot work go go stage happening next hope get around message git organization invitation also want prepare next week mainly searching maybe next next next week something get minder data set try would probably next week exercise go bit towards thinking project let show syllabus schedule syllabus course read see three class already something individual project form also work alone leave leave open bit work alone little bit le time talk project class feasible would make schedule happy form totally really want go alone also support also absolutely fine yeah yeah bit forced free project maybe also come point bit idea individual stay schedule first time many individual protect much bit stay every class either presentation also watch comment discus classroom like drafting ideally data already activity kind kind end idea already consult course quickly go syllabus find zero six addition problem solve find short description related classroom bit like construct project much communication fairly fast try fix data set really important try fix data set topic white quite fast project pitch like isolate main insight many sometimes depending data core project necessarily data analysis course bit course involved every project probably also interested bit exploration find need super deep le known fact even well known fact visualize make story communicate audience also discus like fixing main inside making first make data story kind rave slide deck consult bit go also course next time good consult often sometimes faster often also lag behind big problem guidance speed part term hope meet next week also see motivate come whatever see next time going getting yeah u also problem ah yeah yeah yeah yeah yeah writing time time gauge diamond job description right another used say yeah different yeah cooking yes going infrastructure actually need money see finding always kind strange working good yeah right fund chairman knew write form beginning application function yeah yeah part elective yeah remember typically kind phone another page sign one page say signature someone different right yeah nothing see yeah maybe fine fine said thing bit probably know yeah yeah think thing like waiting thing professor side yeah add three team like yeah manually yeah get bring team\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import docx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Make sure to download the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    contents = \" \".join(filter(lambda x: x[0] != '@', contents.split()))  # Remove words starting with @\n",
    "    contents = re.sub('[^0-9a-zA-Z]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "    contents = contents.lower()  # Lowercase\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "    contents = nltk.word_tokenize(contents)  # Tokenization\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep only valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]  # Lemmatization\n",
    "    contents = \" \".join(contents)  # Join back to string\n",
    "    return contents\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Display cleaned texts for verification\n",
    "for filename, cleaned_text in zip(filenames, texts):\n",
    "    print(f\"Cleaned text from {filename}:\")\n",
    "    print(cleaned_text)\n",
    "    print(\"----\" * 10)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Make sure to download the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove any patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "    contents = contents.lower()  # Lowercase\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "    contents = nltk.word_tokenize(contents)  # Tokenization\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep only valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]  # Lemmatization\n",
    "    contents = \" \".join(contents)  # Join back to string\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove any patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words (customize as needed)\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words and alphanumeric combinations\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove alphanumeric combinations that don't match meaningful patterns (e.g., \"ai4bs\")\n",
    "    contents = [word for word in contents if not re.match(r'^[a-zA-Z]+\\d+[a-zA-Z]*$', word)]  # e.g., ai4bs\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code I've provided is well-structured for cleaning and preprocessing text data, particularly from `.docx` files. It removes unnecessary parts of the text (e.g., session details, timestamps), performs tokenization, and filters out stop words, digits, meaningless words, and duplicates. Additionally, it performs lemmatization to ensure that the words are reduced to themy base forms.\n",
    "\n",
    "Here's a breakdown of how the code works:\n",
    "\n",
    "1. **Text Cleaning (Regex Replacements):**\n",
    "   - It removes specific unwanted patterns (e.g., class sessions, AM/PM timestamps) using regular expressions (`re.sub`).\n",
    "   - Non-alphabetic characters are replaced with spaces (`re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)`), and multiple spaces are reduced to a single space.\n",
    "\n",
    "2. **Tokenization and Filtering:**\n",
    "   - Tokenizes the text into words using `nltk.word_tokenize`.\n",
    "   - Filters out digits, invalid words, and stop words.\n",
    "   - It also removes custom meaningless words (`4bs`, `f`, `xh`) and patterns like alphanumeric combinations (e.g., \"ai4bs\").\n",
    "\n",
    "3. **Lemmatization:**\n",
    "   - Uses `WordNetLemmatizer` from NLTK to convert words to themy base form (e.g., \"running\" to \"run\").\n",
    "\n",
    "4. **File Handling:**\n",
    "   - Reads `.docx` files using the `docx` library.\n",
    "   - For each file, the `text_cleaner` function is applied to preprocess the content.\n",
    "\n",
    "5. **Dmyectory Traversal:**\n",
    "   - Reads all `.docx` files from a specified dmyectory, processes them, and stores the cleaned content in a list.\n",
    "\n",
    "Finally, the cleaned text from each file is printed for verification.\n",
    "\n",
    "## Potential Improvements:\n",
    "- **Error Handling:** It might be useful to add error handling for cases where the `.docx` files cannot be read or processed correctly.\n",
    "- **Efficiency:** If the dmyectory contains many files, I could consider processing the files in parallel or batch processing to speed up execution.\n",
    "\n",
    "\n",
    "the code works as expected, I see the cleaned and processed text output from each `.docx` file in my dmyectory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text from AI2.docx:\n",
      "little bit somewhere intermediate great feel confident might learn ai good know otherwise hope positive background knowledge already available also see room learning success hope help aiming learning quite lot ai expect well gaining solid foundation underlying sure cover extent let try let try see definitely talk ethical think pretty much find small machine learning speech recognition well forth couple fix interested rather broad spectrum technologically metallurgic work well application various like image education prefer final project honest yet made well actually discus little bit think survey thanks lot go bed thinking request final project last week last week last year traditional seminar style way student basically gave presentation collected material particular also hello problem come collected also team break material basically develop last session basically nice predominantly yeah looking ai quite often ai concept always point explaining briefly ai applied health automotive industry different education provided nice overview also work actually get ai particularly also model like thought oh perfect time want replicate exactly thing similar thing thought actually adopted little bit run adaptation found relevant would actually something well adaptation last year similar last year nice actually team material course session also time would like result presentation report write also would like something four day one would said something tangible probably tangible would like yes kind code product applied gallery one article would go tea church think something always two one aspect basically project work together scope also learn lot working together slight tendency say three used good number time experience already tendency maybe one person working le rest kind might three pretty efficient pretty bad would nice maybe agree common umbrella fee probably necessary something thought well might nice something rather also exchange also class little bit one might complementary something would say interest topic sense little bit plan general idea yeah think well reduce time next three maybe little bit build would leave form going assign somehow leave yeah come proposal maybe might write baby class find common probably let talk differently one thing thinking aiming pushing direction one thing thinking actually really situation also may point view university like construct university actually could use ai improve yeah various lot somewhere purely administrative education classroom education also recruitment went also recruitment process know probably kind yeah maybe would potentially ai semester course could plenty one somehow common theme university manage step basically include ai framework couple top could easily designed question also would whether yeah need slash possibility actually align also maybe registrar service office say would like something pretty much recruitment team say oh going maybe even least final solution probably think would typically beyond call kind prototype say something could simple check example could answer student train information whatnot trying lose typically would like see project well university also kind scientific approach getting prototype running would also relevant kind systematic evaluation stick example designing student recruitment keep touch potential kind evaluation whether prototype actually successful satisfying need also evaluation validation point somewhere really deal laid pretty much beginning say take well typical example would say nowadays ai lot many say available fundamental fashion large language big trained going train large language model race expensive would like would like adapt large language model maybe general purpose tool specific would like fine tune data really specific theme talking also wan na get gift student answer student yeah undergraduate computer science want give student answer valid maybe would like specific answer particular university place therefore read eat adaptation correctness answer certainly aspect use evaluation might well maybe engaging also conversation easy yeah fast might one evaluate could think would reasonable project topic would say one hand clear cut idea kind ai method tool use context applied evaluate produced probably done certain prototype level fine limited amount time semester basically built immediately also issue question actually get background information oh additional point probably might able address also might able address know ultimately system would overall infrastructure university would side actually run another question also typical ai implementation usually setting assistant scratch well infrastructure various running decide point oh yeah add see kind system maybe available well yeah information available require kind need actually extract handbook information example something yeah would sign also project say something cover know need link need established want roll final product side find project could look like would hope certainly schedule would typical thing expect maybe end submit short proposal would like maybe representation plan kind project progress presentation towards probably end final project presentation see another step quest audience happy clear good talk maybe find name might come talk next time good artificial intelligence honest number growth maybe resolve answer definition artificial intelligence would define crucial data something else algorithm algorithmic said learning something action like actually act human acting human proper thick meeting something like accurate yeah accurate probably additional demand put top probably facet intelligence say whatever intelligent also accurate certain degree yeah also connection acting like human intelligence somehow something would say act intelligently another debate start probably moment aiming something action human decision taking human usually intelligent basis expect pretty much level curiosity also level probably fairness ethical expect human expect machine basically probably see dilemma particularly come autonomous driving probably still one prime ai usage nowadays autonomous driving algorithmic action human behaviour also expectation autonomous car must drive least safe human actually yeah might even demanding respect accept make accept happen whenever one accident autonomous car people really put question whether ai working whether autonomous driving ever take might much higher question liability responsible damage well behaviour system quite clear typically expect human viable accident driving car person driving liable autonomous car always still ongoing discussion going liable liability basically programmer point either mistake decide fatal accident potentially need algorithmic decision hit wall driver hit group one well foresee might occur design kind solution flip random coin randomly car either wall group complicated ethnic group elderly people whether difference getting one person multiple decide something ethical dilemma liability also right decide might also culturally different better appropriate kill child take account child die take account elderly person whether might different towards large kind liability necessarily question ai per se come addition matter yeah matter accuracy end also expect multiple basically yeah know ai system say might produce least debatable let put way might gone definition ai clearly certain degree aspect personally quite often used term automatic decision making ai although decision making might already restrictive ultimately transform pretty much every action also decision probably true even write sentence generate also bit ai generate text boil say oh always decision next word sentence district decision making probably people might say decision making restrictive limited potentially action human like action decision making decision making always little bit yeah rational structured probably lived human least quite often also go next slide see found page simulation human intelligence think act like various cognitive abstract thing learning reasoning problem perception language comprehension definition also cognitive technology actually able problem probably one thing always foreground problem decision making rather another certain degree perception particularly language speech comprehension yeah actually action understanding part historical still might probably complicated reason really give argument certain done way done certain done way something also lot development probably still need done yeah since 1950s slide yeah different basis development ai historic go back basically ai went number different phase also time also different focal problem really beginning idea construct artificial neural really mimic neural system brain human brain message slash machine way rather simple artificial neural binary artificial neural could say yes like general structure neural laid ago take jury think really solve open data would like various difference yeah one hand technical side kind assistance language side general also going away binary logic yes fussy eight logic binary going basically face 1960s first rise ai many time conceptually great really put practical real time yeah power limited also good practical implementation nothing really could say great application led certain decline ai winter basically much happening middle 80 basically time also name ai actually admit neural network back propagation prediction based network popular feasible take time headline data analytics data science much headline ai probably ago still ai popular even would say yeah data science probably shown sometime famous quote barry time chief economist said next decade would sexy job already later people statistically later probably would call ai developer ai science fundamentally would say well fundamentally good question underlying conceptual three different ultimately dark age ai early ami pretty much use lot data structural technically built formal logic saying logical system would like based logic would like develop system like human rule based system striking meant actually combination yeah exactly beginning middle 90 instead aiming finding rule system give idea basically always say combination huge data corresponding power system somehow rightly together say crease success also increasing quality guess one hot look ai interesting thing like quite number request fully comprehensive clear moment business landscape much ai supportive variety one hand certainly nice success data available technology available addition quite number air force short always driver labour force scarce think replace driving one try somehow group different clearly lot ai might ask question difference data analysis system ai system example data analysis pretty much difference data analysis system ai system might small area classically credit apply loan quite long longer period time already used credit scoring system internally information whether back typically ask new apply new loan certain regarding whether house whether much money banking like married number apparently past positive back paying back many straightforwardly interpretable like yeah someone married family another le likely simply disappear easily possible higher chance paying back people involved another sense responsibility maybe well obvious say principle information already informed time taking account human making decision point basically automat scoring system still end human took decision said score bar also already ice system would ultimately also take decision basically also ethic well actually already previous time become prominent ai system human decision taker bank also fa give loan person giving loan person maybe also reason people actually happier point four yeah kind compute core person bar like braiding usually two different sometimes grading one thing go read report thesis admit often anyways first thing read thesis overall impression say oh excellent thesis good one good one good still pas overall idea grade going usually also grid put say aspect maybe time two luckily align borderline say oh good thesis good thesis somewhere always helpful well easier make decision say oh apparently scoring came something despite fact somehow torn example decision easier understand also human typically number go along decision easier corresponding know hand automatic system far guess exactly also point maybe question difference artificial intelligence human intelligence human yeah somehow think might even take account many might able know student know kind received might say oh maybe harsh great high slow somehow modify decision take kind reasoning guess take account probably beyond hope debate whether actually good additional taken account additional taken account taken account certainly also potential negative bias also potential positive bias kind reasoning probably rather complicated machine actually pursue woman might one separation human intelligence still little bit ahead human able ponder reflect certain maybe really foresee various might happen might better maybe hopefully better system probably also intelligence human least might capable maybe still majority capable well might one clearly one broad ai close come customer service probably application also large language success speech recognition large language last really field custom service extremely extreme yeah certain degree well somehow based data specific field basically last language opportunity understand text create text generate text communicate based communication machine rather predictive analytics closely related top point state analysis clearly something also long time probably also former time well data analysis prediction probably made done specific time made quarter maybe predict future basically every second automatically done time continuously respect also aspect ultimately personalization probably also much made depending large language also depending much data inside well say based user behaviour people done based maybe also social medium information people somehow expressed would like kind different together somehow much easier product around user probably aspect different somehow come together look course ai business society let also look little bit business societal information business aspect clearly eye much also field kind largely come mixed decision making yeah kind treatment done also come try rather treatment plan patient particular better treatment example nowadays based number really many really see cord right whether decision making somehow still think also sector reluctant say oh decision taken eye system would always say yes yeah underpinning support ai system fine decision doctor well patient would like forced one treatment system something like always ultimate decision medical professional whether want also decision patient whether agree treatment least long one sometimes also feeling actually ai used really content necessary say oh opposed decision making system education also yeah throwing area probably indeed decision making probably difference education prob nobody people worry decision taken system kind force material ai system lot information discipline field included basically ai system based individual performance test questionnaire whatnot next session going get next information session kind text getting kind presentation get multitude covered maybe topic different format one presentation one presentation another professor last one maybe video comic rather entertaining way might different covering content different people individually might react differently different presentation might learn done learn seriously expressed people different would say long ultimately successful nobody would care whether decision got one watch nobody would care education automatic system whether get one drug people say oh want human loop ultimately decision probably say let work successful happy though kind relevancy probably true probably impact whatever treatment get anyways never know life based context probably decision taken machine weigh usually people weigh le health care transportation yeah yes one visible autonomous getting calmer moment much five ago even three ago guess last two rather silent work 1st legal liable fully autonomous driving also yeah think mixture work work cop also reason starting basically lot autonomous driving going happen near future already happening brown transportation airport something either easily ultimate driving restricted space basically limited access clear enter unlimited number potential traffic situation little street lift kind well airport basically people anyways move gate bus aeroplane think maybe change runway believe rather conspiracy real traffic actually probably really easy easier city traffic german city probably much easier row traffic already chaos grown used ultimately criminology society security probably one big societal well lot automatic rain whether lot come also clearly repeat public fear pretty delicious issue clearly ai also lot dangerous guess point make break little bit confused said say regarding time continue second round hope agree despite written differently composite moment sort already announce next week away conference next week class anyway two hope sorted correctly ideally would start vote let go break rejoin afterwards\n",
      "----------------------------------------\n",
      "Cleaned text from Vir1.docx:\n",
      "mine last week seminar prepare quite stressful semester new even confusion getting registered something like least good found already registration also class today start last year basically right visualization communication visual communication data story telling let share know already screen provided another organization found problem le technical reason invite blocked try invite today get hopefully everything worse work idea get invitation today little massive probably one sandbox positive also project similar data science lab course also provide repository one information go page already click syllabus site think yeah hope many already rough schedule precise maybe make precise internal rep course organization repository yeah general structure already maybe adjust bit depending happening come bit later hopefully toward end today session find think also link page think also provided encompass net also get link also team hybrid one guess maybe arrive try provide hybrid setup make clear course designed person presence session think four something like individual bit similar also course lot main assessment project work may make sense direct interaction individually project work think hybrid participation work hope end sitting alone entertaining screen yeah also session really work think example today session ask yeah discus neighbor briefly discus bit together principle may organizable much harder easy spot yeah need try offer hybrid setup way work yeah think possible best setup let show today get topic end like yeah today visual communication data storytelling let go right contact yeah somewhat lot communication usually think verbal communication mostly mind reality lot communication visual provide something visually table whatever even 3d provide build something u communicate also part communication think communication sending information perceive maybe perceive differently maybe perceive maybe provide complicated way complicated decode generally come place provide visual say data science work yeah go super deep like general introduction saying people remember much hear much see really sure come yeah think striking lot remember better yeah type visual communication start probably also type visual communication leaving type also many factory communication yeah leaving factory smell lot visual sense u would say way think visual perception sense second really full knowledge question really second vision another smell touch think hearing get information course smelling touching also important special touch far reaching right touch wall side maybe hear something think something perceive information something would say structural difference vision hearing work somehow far reaching hear someone something far away see something far away structural difference look store information record data visual data sound data structural difference yeah something yet anyone idea bit huh yeah converted something also audio format think dimensionality see picture many hear noise dimension one sense right also directed vision look sense hear backward first really possible technically possible make sense right much yeah linear directed vision watch around want also talk linear way go step everyone follow order saw visual everybody look still course structure visual perception maybe people look thing first something like want look bit learn bit think also nice topic yeah practice better data work data visualization work create figure create figure audience look first type anything slide need discus visual communication early even cave let say ago somehow visual communication like many show alphabet really visual communication alphabet textual communication let say speech also write becomes also visual communication type writing become also crucial go deep people really typography yeah think also interesting field let go deep statistical visualization graphic design nowadays even video even advanced modern form visual communication course mainly focus statistical course somehow yeah surrounding general visual communication good much general go visual communication first question read scientific paper maybe 70 usually read want get information paper read first probably title right huh could give something yeah right yeah absolutely yeah scientific paper abstract often executive summary rest step linear way read complete know ah read conclusion go back go back beginning see stopped think conclusion interesting enough yeah also often course skim right try sense maybe show think fairly similar think also bad like sense thing skim look try perceive message whole article maybe get already idea telling try read figure caption therefore often informative lot dialogue article sometimes caption really long dense information often like lot get detail especially know already bit already want background information think caption much red text middle paper introduction start normal reading maybe conclusion emma saying sense really even read text often sometimes lot sense even always case use visual communication say jump back forth also reading perceiving text also yeah super essential think many spend much time craft article much lot time much text would say least course also writing text lot time also need done care go idea want compelling bring large part message around write text somehow bit even around quite always formal word count course want lot text generic want information condensed graphic cool much value think also generic text word count really looking although somehow need formally somehow look slide read everything slide work way read first read like nine read first like one yeah skip something anyway main headline fully precise way use huh yeah see fully precise go top bottom right go way sense first reading subtitle good idea sometimes interesting main generic anyway read main point order fully precise order control fully graphic something like design fully control first control emphasize somehow want go visual communication let remember get back time make break right yeah talk neighbor like discus bit something short 3d data interactive graphic even page data idea next two hope give framework also provide discus together next two class side go bit already something mind think remember next may want look let look bit detail sure best choice read see tell need yeah see also ready go see slide show view edging linearly cover actor mean yes tier bluish edge time deer example right see another staging right yes technology getting connection yeah think really think pattern continued found twitter think good sign even bit like red cut road like maximum point yeah late turnover maybe get go deepen depth point really way statistical graphic also mean memorable say nowadays come back come back come back minute fancy type visual communication 3d used everything 2d maybe produced 3d data visualization probably found quite hard comprehend typically advise use 3d paper 3d really 3d found yeah day ago exhibition park anyone knew park museum nowadays spot rural area code working break enigma code maybe saw movie imitation game nice story alan founder one founder breaking big part yeah big mix mathematics computer science linguistics everything also museum also exhibition art data think see actually know exactly orange white checked probably maybe inner town would think buy one probably right quite much new york think city large much yeah think site probably think even bit lower although still quite large sense living premium city already support much smaller hong thank think also video much even smaller video watch another video similar topic anyone know video even better even know would anyway shown prominent video think take another data visualization course necessarily video story telling u quite common one yeah know exactly yeah quite important data visualization video unfortunately already dead let listen visualization right heart work teach global health know data enough show way people enjoy understand going try something never done animating data real space bit technical assistance group go first next health life expectancy access wealth income per person four six rich healthy going show world ago come brown red middle east green south blue yellow size country bubble show size population pretty crowded sick poor life expectancy slightly better much start world industrial revolution elsewhere move away rest stuck eventually western get slow show impact first world war flu epidemic catastrophe speed 1920s 1930s spite great depression western forge towards greater wealth health japan try follow stay second world war stop bit look world great year war topped medal table winter born difference world ever united front japan catching brazil way behind getting little oil still short china still six look happen go lifetime former independence finally get 1970s catch western stuck civil war hit see world today date statistic people today live middle huge difference time best worst also huge within show country split take china split go shanghai wealth health today pouring line apply show like split rural like yet despite enormous today seen remarkable progress huge historical gap west rest become entirely new converging world see clear trend future aid trade green technology peace fully possible everyone make healthy wealthy corner well seen last story shown beyond involved plotting pretty neat huh question mark 18th century china five done probably course quite unsure really think try estimate probably various way probably already time course also inflation whatever measure unify 18th think lot work involved take probably maybe life span data bit easier get income data also probably quite hard come yeah right think per deep economics think many say per basically transferable average income huh per year per year yeah per year yeah probably per year yeah typically per year statistic yeah typically yeah know average income data let say information way easy digest simple graph think purpose main message say yeah one face message mainly good better often think message somehow longer perspective let look video think another guy get back also seeing right minute another guy made video might also seen like one income along axis life expectancy along axis different color size mirror number might seen last two ever increasing earnings buy seen true say something important world today however used propaganda justify state hail western capitalism ideal society extreme like completely take look instance really world like look u neatly small little bubble small mean income reality national income unequally distributed among bottom half population average income per year much lower national average half u income roughly four national average income increase understand look entire distribution top zoom bit among top average income million listen carefully since share national income going top almost twice large share going bottom half population whatever huge impact national average last top income half population poor national misleading like affect long people live woman belonging top average life expectancy whereas man belonging bottom average life expectancy mind gap yes gap u exceptional respect zoom come relatively egalitarian society take tube well educated road le educated voice record price take arrive life expectancy improve year bit longer watch full want time video yeah problem average right everywhere whenever people talking average use side story want say right yeah every dot much one value two dimension think good point said reality different course bit question really reality course average also reality lot main point average know inequality say right way also put information yeah think clever data visualization probably possible put everything course yeah showing inequality way looking data visualization insulin inside average maximum minimum right something nowadays possible still lot work something like interactive right looking one thing showing thereby showing giving sense variation also maybe even variation large saying mostly every country large think true large wealth people income le lot income also anyway usually men also different also another question two video think le data maybe guy let check name maybe slightly data set compare roughly spot bubble looking differently maybe briefly discus come spotted precise better please access scale color size got yeah think much better go right one easier yeah absolutely yeah income zero got yeah cool color yeah also come yeah white board distance like access 100k small distance two something different entire scale one different make yeah right mean really right privacy yes conference different know well message yeah exactly gon na actually maybe realize yeah among think problem yeah think close probably blessing like somehow least mean fast would impress right like stuff yeah awesome let right yes time almost redundant right human right dollar guess could also know right mean always weakness let collect together difference spot back yeah talking like condensed honest really hard tell put income continent country mean yeah one clearly see press blue like easily see one yeah difference technically say looking data yeah sorry yeah left income access go zero 40k right go zero k meaning left everything put right one everything close entire scale latter right yeah sense income progressively higher see like dot 90k income yeah everyone see one freezing dead think would better become anything true would see size better think summarize table soon yeah two think essential part missing scale got already different scale think stayed data going bit one reason maybe different counter size background first one quite bright right yeah ground quite see clearly also access think recording modern much good quality something meant color visual see different measure see difference yeah mean colour different color mean basic say color continent guess actually got yeah think color exactly hard compare example maybe remember talking lot would say give one yeah probably justice probably green yellow rest think maybe match although really see orange probably blue red yellow green maybe hard easy spot size everything right size dot mean population right get size besides always question match size radius dot area dot radius area actually perfectly sure get difference area radius population country let say billion china roughly think time anyway matching area would difference let assume let assume radius china radius fund previous four time area wait second million thousand million let say probably like radius roughly radius would happen go change making proportional area proportional radius much smaller much smaller china think proportionally somehow right china would matching maybe fully scale point better measure better visual area population would change correct proportional representation percentage radius would change german bubble bit china bit smaller go small time smaller one go area go area look like think maybe make precise good visual reconsider let go let go china may work would sorry hundred percent right le scale matching square side hundred square side proportional area right much proportional go proportional probably like something like right think better radius area size match something like total population useful area mean area intuitive would say bit harder yeah right construct kind yeah right think reconsider remember important topic think proportionality aspect area much better typically choice yeah think fit know fit go one dimensional measure large get way large say square everything right go area square everything side whole thing area middle question course much right difference much difference much probably best way visualize use size two dimensional object go area mean something square root typically match want make area dot proportionate actual number take square root size think remember probably use soon even something like size area visual typically use like anyway good idea look mass make clear may bit confused maybe forgotten formula right like grab pie always square easier see square root immediately essentially take square root proportional check already break think reconsider point check fluffy look data set behind data set behind number population yeah else life span yeah per let see people income also country continent think come mean yeah right country say yeah also right everything right let first look ross ling maybe remember grammar graphic basis package always start data match athletics look population maybe remember aesthetic maybe come find population size yeah size object size typically size matching continent color country last time right think visual standard think year label another thing static plot actually plot one year right really variation dimension number white number time except year video time point something put directly make movie also always think dimension time course natural one also time principle could also say let go video time weird thing often done label course correct whole see snapshot data static visual filter data variable variation come time break maybe also check population right size different access two rest right x color slightly different axis time something like would ethic would could also something scale several data already limit yes thetic probably sure scale area really double checked guess like compare let say china u think size u four time smaller three time smaller roughly think area probably well right area report le three time area yellow radius yeah radius two time therefore pretty sure area also correct choice already point le zero sorry yeah good yeah zero roughly income zero need left though yeah right yeah right roughly yeah probably bit yeah something beyond right also one important difference yet scale idea reconsider fact artificially extend could really want full range difference need cut get would think still variation would look different variation right white income also different scale income data yeah like one linear one like yeah least first let take logarithmic right important first recognize linear right go time let go distance even time logarithmic scale sense time yeah plus k plus 20k plus 20k time typical logarithmic scale logarithmic scale still original value often quite useful data visualization yeah probably scale logarithmic sort cold nice still interpretable something quite important often data visualization make technically data exploration often take logarithm plot look easy interpret background knowledge making public lot sense like course also importance say people time yeah dude time le time le much yeah absolute lock scale linear scale could style transform data let take look data let say cloud like need like bring graph somehow flip axis mirror like go yeah also feel x mirror along axis also switch somehow right bring bit also say right way data thing slight different color scale think know essential make data quite different graphic course even change graphic play first entry point conceptually thinking present certain way let discus bit type short break great oh yeah thing like coffee line yeah get something yep sorry yeah registration tried registrar problem least yeah need two data science register miss requirement yeah wine could sense great right think like read unfulfilled since know past already create trust algebra yeah want probably right yeah see series confirm like error side also yeah also push german word past missing yeah also great yeah see yeah communication unfortunately possible yeah go office see yeah send see maybe back confirm know anything solve think everything done even say like weird error think done know time come add team probably right yeah maybe probably soon way also got already information contract think worked already old call think mine new think hope contact within week hopefully could start 16th anyway thing student bit think bit think able really start work student coffee like slow see would work think even feasible one hope contact start week yeah certainly want step guess thing bit cost center central fund big course made request assign bit later hope guess week maybe put ask guess maybe hope week come short notice course imagine probably often like maybe time year would good yeah lot think lot peter process typically try make exactly state 16th starting hope work think come information weekend fast ready think work let see contract conflict yet course yeah think still normal quite tight schedule see use believing two year two next posting contact maybe even could get everything still yeah also somehow still missing yet also new site think basically think bit maybe something moment like fine find work silly well already work even good one already late right even yeah exactly yeah think continue anyway next story probably adult go super deep graphic skip back come next topic otherwise need watch video still running yeah build think got bit also sense different political message say right one see progress world development also world development data say sense german course right know german book something yeah like right huh right data yeah link left think political yeah political politically right yeah like yeah let think idea look data different side see different depending present mean know end must home see want see right yeah see want see point yeah way short fun fact interview author said gen z people got title wrong starting data german word date someone could also read left wing people dating right wing people data yeah one graphic book elephant graph yeah elephant graph see percentile global income distribution top bottom income distribution people probably side even poor people rich people probably also except super rich scale maybe top world income something state change increase percentage think percentage change real income top five top middle say population change percentage certain say gon na say think message graph increase income almost everyone part among poor world among middle poor among middle say middle income middle class upper middle class really upper class message graph highest increase income like supply people course really yeah one second think grease u site yeah actually case right one something like upper middle class world really super rich also super poor increase lower middle class fully poor still poor highest increase think necessarily left right think type people could use political message say message could propagate honest message think middle class people informed different way really income right section one talking people middle yeah one yeah one great maybe baby somebody go purpose political policy might really affect right site lift right yeah could yeah idea political message tension majority say income increasing ignore like let phrase bit concrete want political message suffering would given graph whatever political message want say one though make elect something would message people range right perspective increase income last year people poor people much increase income suffering could negative political message trying motivate people stand sort saying upper middle class losing sense could message course also message also suffering anyway super poor really suffering people course question want address people say suffering phrase let take super rich lot let take need support much increase right could story operation want decrease difference yeah try like grab attention population say going make income higher yeah rich like yeah right support trivial right access percentage income class time know absolute value know absolute know bottom top five already hard rest income increase percentage increase would say last gain last people could political message people come late confirm even break possible get coffee get actually thought somebody next would go deep bit sight topic similar see similar data different political another example complicated graph elephant graph mainly saying let see yeah think graphic often used vestal u middle class send suffering world rest little class see middle class middle class much income data politically used way yeah course somewhere let look another version graph one black one elephant people see upward trunk yeah bit one famous hockey stick also used lot climate graphic temperature increase hockey stick going super much red really red black red idea easy grasp income upper middle income difference axis red one absolute gain much see different picture communicate black one yeah belong highest income see crazy increase yeah lot absolute people may percentage let say absolute yeah right saying yeah black graph interesting message sense people lower middle class middle class middle class total world say total income wealth still tiny absolute even people total increase even increase small increase percentage wise nominal even lower class get point really someone lying graphic issue lying statistic people say often anyway lying thing look data certain way necessarily wrong interesting information complete picture always good different looking data one question know graph know graph data infer graphic technically know think quite backward probably behind guess think information data say infer way typical could learn ask data think black line know change axis plus know lower people know lower essentially one infer know percent even know income distribution know bottom know much also know much guess course probably school distribution probably little absolute value average income group average income group every income group probably also logarithmic scale probably multiplying million going always looking percent often quite interesting way also quite data wrong room know let see black relative income growth meaning percentage growth guy hit percentage change look right look right yeah another topic also come often data visualization task axis one axis absolute change one axis percentage change typically advisable may interesting compare certain normally quite dangerous particular meaning point magnitude arbitrary scale ax right therefore must course compare factor see large elephant absolute relative increase income see absolute little compare two read properly quantitatively look red black basically yeah black line used say western middle class much last look red line still see poor people still quite cool gain still much u natural percentile two graphic left wing comically left wing data politically data different way truth complete good look let see time also talk bit course maybe still time let discus second point bit without maybe let assume data five relative absolute income relative absolute income change relative income change let assume raw data raw data would data set really people world five billion year let say billion year assume data could make red black graph need process course data bit problem income compute question data transformation need yeah several one mind come one need exercise course right get need mean income growth need divide income year n income year n get income growth right let say yeah get income change yeah need divide income one think better yeah think broke right yeah ideal data format right yeah want new data form thank need go transform way one yeah hear compute percentage change yeah use pivot type right need individual much course people dead people new labor force probably fully work need right step fine need one step right want also want income change individual change present access yeah right oh good principle good idea one thing need change first need compare first compute yeah year need condense data set million convert something like per cent tile let say five percent detailed want probably zero five percent typically want even smaller top world work top whatever percent percentile something like income average median income group know compute change yeah dividing would say dividing yeah get percentage change divide change another relative change enough absolute change would something else right step right real quick two already highly problem bit reduced bit hard conceptualize least need people become whatever easy process several happening meanwhile also income distribution may lot maybe income become much unequal much equal meanwhile would absolute drastic see graph looking full context lot background knowledge course could also first look distribution income something like hidden still interesting see often think income often thing start present please yeah think often happen somebody one lower percentile many make percentile one upper percentile also yeah really know think typically normal trajectory people get bit overall life span become necessarily yeah know actually much yeah also know saw also example would interested see differ right maybe distribution exist maybe know yeah also know exactly right let move point sometimes graphic skill data scientist practice basically time graphic see data could often quite hard work involved often certain message also interesting decode need data probably data really wrong best estimate probably quite hard still way also specific purpose yeah already conceptual work quite valuable tool able data scientist criterion come soon let look dashboard maybe already yeah seeing say population permit edge yeah like edge permit bye yeah see edge distribution men guess right last horizontal path people course right yeah much raw data say count people male million many female say see two also two ax quite nice put two maybe small see access one zero alright year course one minus edge course relevant information see people around first year around good compare course absolute easy compare add see almost bit parallel see live longer right see leave right side yeah see especially older bigger could see look manual bit bigger clear see image complex yeah think produced mean red blue similar minus right basically happening whenever make radish whenever make number already think nice trick much anything general perception clue compare let look also time remember time natural thing let start 50 year born eighty old born going go let see every year born year see increase right success pretty yeah western u think baby know exist world come minute already coming maybe born already know exactly see check cohort compare lower though somewhere come daughter something good actually projecting color seen mean making yeah right also color change color slightly change future projection clearly see projection something know flaw projection real positive think really chance getting right yeah let discus minute know access graph maybe also put want come back god full screen get short input know interactive graphic many interactive graphic real time data coming business data dashboard quite commonly used quite common could also quite common job data side side also side reading making informed explaining something like want mention something relate also relate course main part today look basically certain point practical relevance nowadays many data quite common thing one question graphic need hierarchy simple short go want perceive data structure data hierarchy first start basic go quite relevant today come minute another source visual communication call multiply page data think saw page outward data already visit find good typically data interactive democracy index select various well also time maybe also bubble rustling think much transition tradition positive guy two also want map world want make u overlook positive general often look negative current classical success typically literacy rate around world childbirth see basically lifetime declined tremendously child age declined tremendously last basically almost another thing already go data storytelling let start let go data storytelling last part let check organization data story data story go bit beyond visual use course also something like main story point main point solve graphic twitter think side link story three tree core basically related current news think two ago broken four graphic confirmation woman turned looking new data still confirming theory upper limit point one visual core story current news say another meaning story mean context yeah fully functional data story provide context pause yeah right everything story come provide compliant yeah rang data typically make much meaning sort present visually way still meaning like bit think also bit mismatch meeting story data basically great data story get mean happening know think mean yeah right almost yellow kind really match right yellow also green special green composed thing sense think need general point interesting course main point data basically nothing get meaning technical get much meeting get much meaning right decomposable anyway last group work think try get meaning phenomenon important different right thousand current almost current future projection propose yes bottom line bottom yeah give thinking age speak two world maybe also part origin also many people let say let see take group question quickly discus make story question take b question story two 1960s nineteen story area large dark red area story gap maybe two often gap going say story happening first step happening 2nd step maybe idea happening discus let say go got question know bit think really nice right yeah additional war also flu yeah right bit related directly question unfortunately see birth well think important maybe help correcting let know want know think see age course come certain year visible see main line move interesting got get yeah think let see yeah want go thank young man right yeah pretty much like watching yes know gun right yeah know honest like get like know maximum age know movie yeah father yeah go yeah actually yeah go group mark around group b around two right would around like direction one generation cool lot find man course next generation right front runner think yeah god first one person know thank take nothing choose story mainly see maybe someone thank much make talking right look like yeah like make yeah see eight close yeah could read suspect might found yeah actual story well roughly let start saying group see could listen please stunning booklet see doctor red population since world war two also analyze range zero five see shrinking point likely lot widow think le possibility finding match le region right one generation lot find man surplus since many men apparently felt war next generation gon na right basically seeing already projecting bit probably also true main point much first question typically born look currency old typically age group either equal even slightly mad much also yes quick question think making right basic question many much lesson right wait le men yeah nothing worse give go bar time much le yeah mainly mail missing typically time mail missing front war age woman work start yeah worst five missing yeah year typical right know german war history better even yeah end time war even young sent people dying probably consequent missing see gap course get transferred upwards yeah come bit closer normal excess see also without reason board want interesting question maybe already next would want repeat go question two mean said generational effect yeah lot find man vice guess see drop childbirth course next generation yeah guess right actually access mail place many big unless excess le place right drastic gap starting maybe related yeah let ask let ask think explanation ready gap course gap think gap right later guy gap destination basically two go two something right two come like come men join army like effect potential maybe lead say birth maybe yeah see second gap slightly bird rate like yeah simulation get yeah select book baby boom generation yeah baby boom generation really clear saw best basically age tight gap narrow five essentially three something exactly said war time gone front even family wife whatever could produce front maybe also yeah maybe know contraception pill available time still could also contraception way time also availability extreme actually mother think produced front holiday guess ship mother telling father grandfather barely know came back front something daughter saying grandfather photograph board father photograph real person fine yes see generation one gap second first world war right first blood board phenomenon four much front home front say yeah anti baby right know like year pill rock yeah drop yeah sexual revolution right interesting point want hurt data yet hurt inventor pill death get talk said yeah story bit pill japan drop le year without large scale access build think japan well story go pillow late japan remember exactly also funny story let go next gap yeah boring year gap born gap yeah one yeah saw 70 also may brewer maybe east anyway also west yeah graph low birth rate yeah sure low birth like well yeah slightly increasing late mid year bit let say generation actually gap slightly one thing course yeah abortion also contraception fill also explaining lot decline another even simpler story education even simpler story typically typical age get let say average year first child even let say think left people later also bit le generational maybe story generational transition get expect get another gap later le typical age child around think even see even see gap bit washing right yeah generation even another gap let pronounce even another gap let pronounce small increase think still nothing sent watching exact get child course lot rain something final group open tell u e virus yeah like see kind see first looking everything school yeah actually really sure corona effect maybe large enough maybe kind really yeah small entire pandemic gon na maybe took year pandemic want burst think affect yeah year yeah 1st know like maybe le first people born pandemic know still alive yeah projection steady decline say pyramid standing top really portal said right yeah diction yeah generally phenomenon yeah phenomenon happening almost development demographic transition first child death decline worse population somehow trilogy go solid around saw 70 think still nation population china way think still growing see already part like many still like still like actually anthropologist generic pattern still real super hard scientific explanation generic fact happening place general rule behind also good well done think bit way data storytelling work real data relate historical explain story around happening really see let see time yeah left let come whole term data visualization got already used lot think already hope interested develop towards used lot basic skill analyze new data also essential communication yeah storytelling focus course much getting graphic getting graphic thinking change improve purpose communicating may guess go back data transformation go back data origin think data maybe transform somehow make certain point transform explain example logarithmic transformation really necessary think necessary show best show locked probably show actual kind another technical chance increase look graphic next week basic made maybe look data science lab maybe found possible let see yeah point another thing technical go really want therefore first step develop sense want visualize therefore also next two want look also hope bring send message next day around think bring similar maybe find interesting find world data short visual story say yeah think interesting also lead already want make data story project future maybe bit fast think show also bit g plotting working python think transferable even interface python use mud plot lip show good example common really visualization saw lip course mark probably great graphic also bit technology way thinking grammar graphic style aesthetic transforming scaling way speaking say also graphic like really made bound due basic thinking lot sense also think course principle exchange right also continuous color map put color could play see sense message term also lot break use ah another interesting point new taken book yeah good provide book course organization repository access provided book found clear indication illegal get provided absolutely sure also know library idea read book course chapter indicate syllabus chapter super strict also provide syllabus think close reading important example two say mainly optional skim interested course think read everything book least blue many clothes think stuff generally interesting also may valuable written bit business perspective know much business person think perspective book also valuable also valuable general data storytelling want make project bit businesslike fine want make project societal phenomenon societal aspect many similar rustling whatever also fine yeah lot flexibility start thinking want yeah book work maybe want also input data visualization data storytelling business think also good resource two prominent also good stuff also quite practical learn maybe select tool go many around think time next look starting bar density box whatever really fully bubble like rustling many different good look bit around get inspired try think whenever data look could make sense try something experience even quite advanced technicality technical way still often idea try look good like start work thought could put another aesthetic put size work thought lot also communicating think lot trial error trying new would happy go try discus class would happy provide yeah also course think python right yeah think way quickly least try bit also inspirational could right technical could look much time still bit something soap showcase final step data also much business world book storytelling data typical challenge said visualization green direct red indirect really sure actually different company time running year close deal axis along close deal goal day see sometimes day time le day difference direct indirect way could decompose message business presentation may better way show information waiting two let say sense eliminate clutter remove important focus attention focus something like question bit clutter could eliminate get yeah day decimal science remove axis one yeah plus go away yeah also entire year remove look sitting yeah caption would remove text top bus well one right also go away think yeah number know yet maybe keep yeah change many yeah way good time bit lighter restart blue maybe light blue gray something yeah good point distract still give opportunity check clearly sometimes important yeah always want close deal maximum day yeah right highlight maybe exceeding day right sure yeah actually right wrong try think better solution also different type visualization bar chart design right yeah think also found often easy compare actually want see trend green red compare also proposal put line indirect problem every data point dot dashed goal line something reference line direct problem idea maybe compare learn direct faster indirect think something particular june happening actually know idea business case come also bit technical time come later also something keep mind would never type clear story point course end still lot work go stage happening next hope get around message git organization invitation also want prepare next week mainly searching maybe next week something get minder data set try would probably next week exercise go bit towards thinking project let show syllabus schedule syllabus course read see three class already something individual project form also work alone leave open bit work alone little bit le time talk project class feasible would make schedule happy form totally really want go alone also support also absolutely fine yeah bit forced free project maybe also come point bit idea individual stay schedule first time many individual protect much bit stay every class either presentation also watch comment discus classroom like drafting ideally data already activity kind end idea already consult course quickly go syllabus find zero six addition problem solve find short description related classroom bit like construct project much communication fairly fast try fix data set really important try fix data set topic white quite fast project pitch like isolate main insight many sometimes depending data core project necessarily data analysis course bit course involved every project probably also interested bit exploration find need super deep le known fact even well known fact visualize make story communicate audience also discus like fixing main inside making first make data story kind rave slide deck consult bit go also course next time good consult often sometimes faster often also lag behind big problem guidance speed part term hope meet next week also see motivate come whatever see next time going getting yeah u also problem ah yeah writing time gauge diamond job description right another used say yeah different yeah cooking yes going infrastructure actually need money see finding always kind strange working good yeah right fund chairman knew write form beginning application function yeah part elective yeah remember typically kind phone another page sign one page say signature someone different right yeah nothing see yeah maybe fine said thing bit probably know yeah think thing like waiting thing professor side yeah add three team like yeah manually yeah get bring team\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Display cleaned texts for verification\n",
    "for filename, cleaned_text in zip(filenames, texts):\n",
    "    print(f\"Cleaned text from {filename}:\")\n",
    "    print(cleaned_text)\n",
    "    print(\"----\" * 10)  # Separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I decided to use the textual data from the .docx files. I started by reading the files and preprocessing the text using the preprocess_text function. The goal in this step was to clean up the text by removing noise such as punctuation or irrelevant words, converting the text into a simpler form that would be more suitable for analytical models.\n",
    "\n",
    "After preprocessing the texts, I applied Topic Modeling to analyze the data. For this task, I used the Latent Dirichlet Allocation (LDA) model, which helped me identify different topics within the texts. To begin, I converted the texts into a document-term matrix using CountVectorizer, where each row represented a document and each column represented a word.\n",
    "\n",
    "To ensure that the model accurately identified topics, I set the number of topics to 2 (this number can be adjusted based on the data). I selected this number because I wanted the model to extract two primary categories of concepts.\n",
    "\n",
    "After running the model, I ended up with the following topics:\n",
    "\n",
    "Topic 1:\n",
    "\n",
    "['data', 'nt', 'income', 'things', 'people', 'want', 'time', 'look', 'lot', 'different']\n",
    "Topic 1 is related to concepts such as \"data,\" \"income,\" and \"time.\" It seems to refer to the analysis of data and its use in various contexts.\n",
    "\n",
    "Topic 2:\n",
    "\n",
    "['ai', 'decision', 'based', 'aspects', 'language', 'database', 'autonomous', 'driving', 'intelligence', 'ultimately']\n",
    "Topic 2 focuses on concepts like \"artificial intelligence,\" \"decision-making,\" and \"autonomous driving.\" This topic likely relates to applications of AI and related technologies.\n",
    "\n",
    "Initially, I encountered some issues with the previous code, so I decided to implement this new approach to preprocessing and analyzing the texts. With this new method, I was able to identify the main topics present in the data and obtain meaningful results that aid in a better understanding of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['data', 'nt', 'income', 'things', 'people', 'want', 'time', 'look', 'lot', 'different']\n",
      "Topic 2:\n",
      "['ai', 'decision', 'based', 'aspects', 'language', 'database', 'autonomous', 'driving', 'intelligence', 'ultimately']\n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42) \n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how I arrived at the difference in similarity scores between 89% and 47%, and the reasoning behind the process:\n",
    "\n",
    "Initially, I started by reading and preprocessing the text data from all the .docx files. The goal was to clean the data, removing unwanted content (like session details or timestamps) and irrelevant words. This is where I focused on eliminating stop words, non-alphabetic characters, and some meaningless words like \"4bs\" or \"f.\" I also performed lemmatization to normalize the words, ensuring that they appeared in their base forms (e.g., changing \"running\" to \"run\").\n",
    "\n",
    "Once the preprocessing was done, I used TF-IDF (Term Frequency-Inverse Document Frequency), which is a method for transforming the text data into numerical vectors. This vectorization allows us to represent the documents in a way that makes it easier to calculate similarities between them.\n",
    "\n",
    "For the cosine similarity step, I compared each document's TF-IDF vector to the others. Cosine similarity measures how close two vectors are, meaning it looks at how similar the content is between the documents. The score ranges from 0 (completely different) to 1 (identical).\n",
    "\n",
    "For instance, I initially got a similarity score of 89% between two documents (let's call them AI2.docx and Vir1.docx). However, when revisiting the comparison later, I found a lower similarity score of 47%. This difference can be attributed to several factors:\n",
    "\n",
    "Preprocessing: During the text cleaning process, some important words might have been removed, or the lemmatization might have altered the meaning slightly, which could have affected the final similarity scores. It's possible that in one iteration, the preprocessing steps removed more useful words, leading to a reduced similarity score.\n",
    "\n",
    "TF-IDF Vectorization: The TF-IDF transformation is sensitive to the presence of rare terms and their importance in a document. If two documents have more unique terms that are not shared between them, the similarity score will be lower. It's likely that in one of the calculations, the key terms that defined the similarity between the documents were weighted differently, resulting in a significant drop.\n",
    "\n",
    "Document Content: If the content of the documents is slightly varied, the similarity score will reflect that. For example, documents with highly overlapping words (like \"ai,\" \"data,\" \"income,\" \"decision\") would have a higher cosine similarity, but if one document introduced more specific or different words, the similarity score could decrease.\n",
    "\n",
    "Ranking of Keywords: The top keywords also play a crucial role. The keywords for AI2.docx include terms like \"ai,\" \"decision,\" and \"probably,\" which are related to decision-making and artificial intelligence. On the other hand, the keywords for Vir1.docx focus more on terms like \"yeah,\" \"think,\" and \"income,\" which suggest that the documents discuss slightly different topics. This difference in keyword relevance likely contributed to the observed drop in similarity between the two documents.\n",
    "\n",
    "So, while the first result was 89%, this second calculation of 47% reflects a more accurate and nuanced similarity, taking into account the impact of preprocessing, vectorization, and the actual content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'AI2.docx' and 'Vir1.docx': 0.4787\n",
      "\n",
      "Top keywords in 'AI2.docx': ai, decision, probably, say, yeah, like, maybe, kind, human, actually\n",
      "\n",
      "Top keywords in 'Vir1.docx': yeah, think, right, data, income, like, maybe, bit, let, say\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words and alphanumeric combinations\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove alphanumeric combinations that don't match meaningful patterns (e.g., \"ai4bs\")\n",
    "    contents = [word for word in contents if not re.match(r'^[a-zA-Z]+\\d+[a-zA-Z]*$', word)]  # e.g., ai4bs\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display similarity scores\n",
    "for i in range(len(filenames)):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        print(f\"Similarity between '{filenames[i]}' and '{filenames[j]}': {similarity_matrix[i][j]:.4f}\")\n",
    "\n",
    "\n",
    "n_keywords = 10  # Number of top keywords to display\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    sorted_indices = tfidf_matrix[i].toarray()[0].argsort()[::-1][:n_keywords]\n",
    "    keywords = [feature_names[idx] for idx in sorted_indices]\n",
    "    print(f\"\\nTop keywords in '{filename}': {', '.join(keywords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Similarity between 'AI2.docx' and 'Vir1.docx': 0.4787\n",
      "LDA Similarity between 'AI2.docx' and 'Vir1.docx': 0.0297\n",
      "\n",
      "Top keywords in 'AI2.docx' based on TF-IDF: ai, decision, probably, say, yeah, like, maybe, kind, human, actually\n",
      "\n",
      "Top keywords in 'Vir1.docx' based on TF-IDF: yeah, think, right, data, income, like, maybe, bit, let, say\n",
      "\n",
      "Top 10 keywords for topic 0: come, point, quite, good, lot, work, need, thing, question, different\n",
      "\n",
      "Top 10 keywords for topic 1: yeah, think, right, data, income, like, maybe, bit, let, say\n",
      "\n",
      "Top 10 keywords for topic 2: come, point, quite, good, lot, work, need, thing, question, different\n",
      "\n",
      "Top 10 keywords for topic 3: come, point, quite, good, lot, work, need, thing, question, different\n",
      "\n",
      "Top 10 keywords for topic 4: ai, decision, probably, say, yeah, like, maybe, kind, actually, human\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words and alphanumeric combinations\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove alphanumeric combinations that don't match meaningful patterns (e.g., \"ai4bs\")\n",
    "    contents = [word for word in contents if not re.match(r'^[a-zA-Z]+\\d+[a-zA-Z]*$', word)]  # e.g., ai4bs\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# ----- Step 1: TF-IDF Vectorization -----\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Compute Cosine Similarity for TF-IDF\n",
    "tfidf_similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# ----- Step 2: LDA Vectorization -----\n",
    "# Initialize and fit the LDA model\n",
    "n_topics = 5  # Number of topics\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Compute Cosine Similarity for LDA\n",
    "lda_similarity_matrix = cosine_similarity(lda_matrix)\n",
    "\n",
    "# Display similarity scores for both TF-IDF and LDA\n",
    "for i in range(len(filenames)):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        print(f\"TF-IDF Similarity between '{filenames[i]}' and '{filenames[j]}': {tfidf_similarity_matrix[i][j]:.4f}\")\n",
    "        print(f\"LDA Similarity between '{filenames[i]}' and '{filenames[j]}': {lda_similarity_matrix[i][j]:.4f}\")\n",
    "\n",
    "# Displaying top keywords (topics) for LDA\n",
    "n_keywords = 10  # Number of top keywords to display\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    sorted_indices = tfidf_matrix[i].toarray()[0].argsort()[::-1][:n_keywords]\n",
    "    keywords = [feature_names[idx] for idx in sorted_indices]\n",
    "    print(f\"\\nTop keywords in '{filename}' based on TF-IDF: {', '.join(keywords)}\")\n",
    "\n",
    "# Displaying topics for LDA\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_keywords_idx = topic.argsort()[-n_keywords:][::-1]\n",
    "    top_keywords = [feature_names[i] for i in top_keywords_idx]\n",
    "    print(f\"\\nTop {n_keywords} keywords for topic {topic_idx}: {', '.join(top_keywords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'AI2.docx' and 'Vir1.docx': 0.4787\n",
      "\n",
      "Top keywords in 'AI2.docx' based on TF-IDF: ai, decision, probably, say, yeah, like, maybe, kind, human, actually\n",
      "\n",
      "Top keywords in 'Vir1.docx' based on TF-IDF: yeah, think, right, data, income, like, maybe, bit, let, say\n",
      "\n",
      "Top 10 keywords for topic 0: come, point, quite, good, lot, work, need, thing, question, different\n",
      "\n",
      "Top 10 keywords for topic 1: yeah, think, right, data, income, like, maybe, bit, let, say\n",
      "\n",
      "Top 10 keywords for topic 2: come, point, quite, good, lot, work, need, thing, question, different\n",
      "\n",
      "Top 10 keywords for topic 3: come, point, quite, good, lot, work, need, thing, question, different\n",
      "\n",
      "Top 10 keywords for topic 4: ai, decision, probably, say, yeah, like, maybe, kind, actually, human\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words and alphanumeric combinations\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove alphanumeric combinations that don't match meaningful patterns (e.g., \"ai4bs\")\n",
    "    contents = [word for word in contents if not re.match(r'^[a-zA-Z]+\\d+[a-zA-Z]*$', word)]  # e.g., ai4bs\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display similarity scores\n",
    "for i in range(len(filenames)):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        print(f\"Similarity between '{filenames[i]}' and '{filenames[j]}': {similarity_matrix[i][j]:.4f}\")\n",
    "\n",
    "# --------------- LDA Model ----------------\n",
    "\n",
    "# Set number of topics\n",
    "n_topics = 5  # You can change this as needed\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Displaying top keywords (topics) for LDA\n",
    "n_keywords = 10  # Number of top keywords to display\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF Keywords extraction for each document\n",
    "for i, filename in enumerate(filenames):\n",
    "    sorted_indices = tfidf_matrix[i].toarray()[0].argsort()[::-1][:n_keywords]\n",
    "    keywords = [feature_names[idx] for idx in sorted_indices]\n",
    "    print(f\"\\nTop keywords in '{filename}' based on TF-IDF: {', '.join(keywords)}\")\n",
    "\n",
    "# Displaying topics for LDA\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_keywords_idx = topic.argsort()[-n_keywords:][::-1]\n",
    "    top_keywords = [feature_names[i] for i in top_keywords_idx]\n",
    "    print(f\"\\nTop {n_keywords} keywords for topic {topic_idx}: {', '.join(top_keywords)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Steps Taken:\n",
    "1. **Text Preprocessing**: \n",
    "   - First, I read the content of each `.docx` file using the `read_docx` function and extracted the text.\n",
    "   - Then, I cleaned the extracted text by removing non-alphabetic characters and converting everything to lowercase, so that the focus was only on words without being affected by case sensitivity or non-alphabetical symbols.\n",
    "   - After cleaning, I split the text into individual words using the `split()` function.\n",
    "\n",
    "2. **Counting Keywords**:\n",
    "   - I defined two sets of keywords: one for **AI** (`keywords_ai`) and one for **vir** (`keywords_vir`).\n",
    "   - For each word in the cleaned text, I counted how many times words from each keyword set (AI and vir) appeared. This helped me measure the presence of AI-related and vir-related terms in each document.\n",
    "\n",
    "## Results:\n",
    "- **File: 'AI2.docx'**\n",
    "  - AI Keywords: 82\n",
    "  - Vir Keywords: 29\n",
    "  \n",
    "  The `AI2.docx` file contains 82 occurrences of AI-related keywords and 29 occurrences of vir-related keywords. This indicates that the document focuses more on AI topics but also touches on vir-related terms.\n",
    "\n",
    "- **File: 'Vir1.docx'**\n",
    "  - AI Keywords: 123\n",
    "  - Vir Keywords: 123\n",
    "  \n",
    "  In the case of the `Vir1.docx` file, both AI and vir-related keywords appear equally, with 123 occurrences for each. This indicates that the document equally addresses both AI and vir topics.\n",
    "\n",
    "## How I Reached This:\n",
    "1. **Text Extraction**: I started by extracting the content of the `.docx` files using the `python-docx` library.\n",
    "2. **Text Cleaning**: Then, I cleaned the text by removing unnecessary characters and splitting it into words.\n",
    "3. **Keyword Comparison**: Finally, I compared the content of each document with the keyword sets for AI and vir, counting how many times each set of keywords appeared.\n",
    "\n",
    "This method helps me quickly identify which topic—AI or vir—is more prevalent in each document and where the primary focus lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 'AI2.docx', AI Keywords: 82, vir Keywords: 29\n",
      "File: 'Vir1.docx', AI Keywords: 123, vir Keywords: 123\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define specialized keywords for each topic\n",
    "keywords_ai = {'ai', 'machine', 'learning', 'intelligence', 'algorithm', 'data', 'model'}\n",
    "keywords_vir = {'machine', 'learning', 'data', 'predict', 'regression', 'classification', 'algorithm'}\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    contents = re.sub(r'[^a-zA-Z\\s]', '', contents).lower()\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "\n",
    "    # Tokenization\n",
    "    contents = contents.split()  # Split the string into words\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Compare specialized keywords\n",
    "for i in range(len(texts)):\n",
    "    # Count keywords for AI\n",
    "    ai_count = sum(1 for word in texts[i] if word in keywords_ai)\n",
    "    # Count keywords for vir\n",
    "    vir_count = sum(1 for word in texts[i] if word in keywords_vir)\n",
    "\n",
    "    print(f\"File: '{filenames[i]}', AI Keywords: {ai_count}, vir Keywords: {vir_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this code,we are calculating the similarity between documents based on the occurrence of specialized keywords related to **AI** and **vir** (visualization and storytelling) topics.\n",
    "\n",
    "## Explanation of the Code:\n",
    "1. **Text Preprocessing**: \n",
    "   -I read each `.docx` file and clean the text by removing non-alphabetical characters and converting the text to lowercase.\n",
    "   - The text is then split into words for analysis.\n",
    "\n",
    "2. **Keyword Matching**:\n",
    "   - I define two sets of keywords:\n",
    "     - **Keywords related to AI**: `keywords_ai`\n",
    "     - **Keywords related to vir**: `keywords_vir`\n",
    "   - For each document pair, I count the number of AI and vir keywords that appear in both documents.\n",
    "\n",
    "3. **Similarity Calculation**:\n",
    "   - For each pair of documents, I calculate the number of shared AI and vir keywords. \n",
    "   - I calculate the **similarity score** as the ratio of the total number of shared keywords (AI and vir) to the total number of keywords (AI + vir) in both documents.\n",
    "\n",
    "## How Similarity is Calculated:\n",
    "1. **Keyword Counts**: \n",
    "   - For each document, I count how many times AI and vir keywords appear.\n",
    "   \n",
    "2. **Shared Keywords**:\n",
    "   - The similarity between two documents is calculated based on the **shared AI** and **shared vir** keywords. The number of shared keywords is the **minimum count** of a keyword in both documents.\n",
    "   \n",
    "## Result:\n",
    "- The similarity score between **'AI2.docx'** and **'Vir1.docx'** is **0.2433**.\n",
    "  - This means that there is a 24.33% overlap in the AI and vir-related content of the two documents based on the defined keywords.\n",
    "\n",
    "## How This Result is Reached:\n",
    "- **Document 1 ('AI2.docx')**: Has 82 occurrences of AI-related keywords and 29 occurrences of vir-related keywords.\n",
    "- **Document 2 ('Vir1.docx')**: Has 123 occurrences of AI-related keywords and 123 occurrences of vir-related keywords.\n",
    "- After counting and comparing the shared AI and vir keywords, the similarity score was computed as **0.2433**, indicating a low but significant overlap in the terms related to both AI and vir between these two documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'AI2.docx' and 'Vir1.docx': 0.2433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define specialized keywords for each topic\n",
    "keywords_ai = {'ai', 'machine', 'learning', 'intelligence', 'algorithm', 'data', 'model'}\n",
    "keywords_vir = {'story','narrative','data','insights','visualization','audience','context','emotion','message','structure','engagement','character','theme',\n",
    "'experience','communicate'}\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    contents = re.sub(r'[^a-zA-Z\\s]', '', contents).lower()\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "\n",
    "    # Tokenization\n",
    "    contents = contents.split()  # Split the string into words\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Compare specialized keywords and calculate similarity\n",
    "similarity_scores = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        # Count keywords for AI\n",
    "        ai_count_i = sum(1 for word in texts[i] if word in keywords_ai)\n",
    "        ai_count_j = sum(1 for word in texts[j] if word in keywords_ai)\n",
    "\n",
    "        # Count keywords for ML\n",
    "        vir_count_i = sum(1 for word in texts[i] if word in keywords_vir)\n",
    "        vir_count_j = sum(1 for word in texts[j] if word in keywords_vir)\n",
    "\n",
    "        # Calculate similarity based on shared keywords\n",
    "        total_keywords = ai_count_i + ai_count_j + vir_count_i + vir_count_j\n",
    "        shared_ai_keywords = min(ai_count_i, ai_count_j)\n",
    "        shared_vir_keywords = min(vir_count_i, vir_count_j)\n",
    "\n",
    "        if total_keywords > 0:\n",
    "            similarity = (shared_ai_keywords + shared_vir_keywords) / total_keywords\n",
    "        else:\n",
    "            similarity = 0  # No keywords to compare\n",
    "\n",
    "        similarity_scores.append((filenames[i], filenames[j], similarity))\n",
    "\n",
    "# Display similarity scores\n",
    "for file1, file2, score in similarity_scores:\n",
    "    print(f\"Similarity between '{file1}' and '{file2}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Conclusion:\n",
    "In this analysis, I have examined two distinct fields: **Artificial Intelligence (AI)** and **Data Visualization** using specialized keywords for each domain. Through the extraction and processing of keywords, I have generated word clouds that highlight the distinctions and similarities between these two fields.\n",
    "\n",
    "- **Artificial Intelligence (AI)** focuses more on technology, algorithms, and models, with keywords like \"ai\", \"learning\", \"machine\", \"intelligence\", and \"algorithm\".\n",
    "- **Data Visualization** emphasizes conveying information and storytelling through data, with keywords such as \"message\", \"visualization\", \"context\", \"story\", \"audience\", \"insights\", and \"experience\".\n",
    "\n",
    "As a result, I have successfully highlighted the differences between these two domains through their respective keywords, providing a better understanding of both fields and their interconnections.\n",
    "\n",
    "# Suggestions for Next Steps:\n",
    "1. **Expand the Keyword Sets**:\n",
    "   - To enhance the accuracy of my analysis, consider expanding the keyword sets for both domains. For example:\n",
    "     - In the AI domain, I could add terms like \"deep learning\", \"neural networks\", \"automation\", and \"big data\".\n",
    "     - In the Visualization domain, terms like \"interactive\", \"graph\", \"dashboard\", \"chart\", and \"storytelling\" could be included.\n",
    "   \n",
    "   Expanding these keywords will allow for more refined analyses and provide deeper insights into similarities and differences.\n",
    "\n",
    "2. **Use More Advanced Models for Semantic Analysis**:\n",
    "   - Simple keyword-based approaches are effective, but for greater accuracy, consider utilizing more advanced **Natural Language Processing (NLP)** models. Techniques such as **TF-IDF (Term Frequency-Inverse Document Frequency)** or **Word2Vec** can help me capture semantic and conceptual similarities between texts more precisely.\n",
    "   \n",
    "3. **Comparative Analysis and Clustering**:\n",
    "   - For more detailed comparison and grouping of similar texts, consider using **clustering techniques** like **K-means** or **Hierarchical Clustering**. These methods can help I group similar texts together and provide a better understanding of relationships between them.\n",
    "\n",
    "4. **Explore Relationships Between Domains**:\n",
    "   - If we're interested in deeper insights into the relationships between these two fields, I could perform **correlation analysis**. This will allow me to explore how concepts and keywords from one domain may appear in the other and how these domains might influence each other.\n",
    "\n",
    "5. **Apply Machine Learning Models**:\n",
    "   - For a more sophisticated comparison of texts, I could apply **machine learning models** like **neural networks** to analyze the texts. These models can process the data more deeply and uncover complex relationships between texts.\n",
    "\n",
    "## Summary:\n",
    "I have laid a solid foundation for comparing the **AI** and **Data Visualization** domains based on their keywords. For the next steps, it is suggested that we expand my keyword sets, explore more advanced NLP and machine learning models, and conduct deeper semantic and comparative analyses to achieve more comprehensive and meaningful results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
