{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Analysis of Repetition in Teaching\"\n",
    "subtitle: \"Authors: Nastaran Mesgari\"\n",
    "format:\n",
    "  html:                     \n",
    "    standalone: true        \n",
    "    embed-resources: true   \n",
    "    code-fold: true        \n",
    "    number-sections: true  \n",
    "    toc: true \n",
    "               \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exclusive Summery\n",
    "In order to find duplications and raise the standard of the curriculum overall, this project focuses on comparing the content of two courses—AI and visualization. This was accomplished by taking a methodical approach. Two courses were chosen: one on artificial intelligence, which covered subjects like data models, algorithms, and machine learning; the other on visualization, which placed an emphasis on graphical data representation, storytelling, and insight generation. For each course, a set of domain-specific keywords was created in order to find and quantify content overlaps. \"AI,\" \"machine,\" \"learning,\" \"intelligence,\" \"algorithm,\" and \"data\" were among the keywords related to AI, while \"story,\" \"narrative,\" \"visualization,\" \"insights,\" and \"emotion\" were among the keywords related to visualization.  \n",
    "\n",
    "To guarantee accurate analysis, the course materials were preprocessed, which included eliminating unnecessary text, changing to lowercase, and transforming the data into formats that are easier to handle.**82 AI-related keywords and 27 visualization-related keywords** were found in the AI course, according to a keyword frequency analysis.  there were 216 keywords related to visualization and 123 related to AI in the visualization course.  similarity score of **0.2433** was obtained from this overlap, suggesting a moderate degree of redundancy between the two courses. To further show the distribution of keyword and give a visual depiction of the most common terms in the materials, word clouds were created for both courses. \n",
    "\n",
    "The results indicate that although the courses retain their distinctt , there are significant conceptual overlaps, especially in fields pertaining to data and model application. This overlap suggests possible duplications that, if resolved, could improve the uniqueness and efficacy of  each course. \n",
    "In order to find more extensive redundancy patterns throughout the curriculuom, the analysis could be extended to include more courses as a subsequent step. To learn more about content overlaps, advanced Natural Language Processing (NLP) method  as like as topic modeling and similarity analysis could be used. These results could guide focused content optimization tacti, guaranteeing a distinct distinction between courses and enhancing students' overall educational experience. To improve course content, reduce duplication, and develop a more coherent curriculum, cooperation with instructors is advised. \n",
    "\n",
    "This project lays the groundwork for future initiatives to maximize academic programs by highlighting the significance of assessing instructional materials for alignment and redundancy. Universities can improve learning outcomes, encourage innovation, and preserve student learning by making sure that courses are streamlined and complementary.The insights gained from this analysis are instrumental in achieving these goals and improving the quality of teaching and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instroduction\n",
    "In academic settings, the quality and structure of course content significantly impact students' learning experiences and outcomes. With the growing importance of fields like Artificial Intelligence (AI) and Data Visualization, the need for well-structured and non-redundant educational programs has become increasingly critical. Redundant content, while sometimes unavoidable, can lead to inefficiencies in learning, reduce engagement, and limit students' exposure to new concepts. Addressing such overlaps is essential for optimizing course offerings and ensuring students receive a comprehensive yet streamlined education.  \n",
    "\n",
    "This project aims to analyze and evaluate the extent of content similarities between two courses—one focusing on AI and the other on Visualization. These domains, while distinct, often intersect in their discussion of data-driven decision-making and the application of models to real-world problems. By systematically identifying overlaps, the project seeks to highlight areas of potential redundancy and offer solutions for improving course alignment.  \n",
    "\n",
    "The analysis is centered on identifying keywords representative of the core themes in each course. Preprocessing techniques are applied to refine the course materials for accurate assessment, followed by similarity metrics to quantify the degree of overlap. The results are visualized through tools such as Word Clouds, providing a clear picture of the shared and unique elements of the courses.  \n",
    "\n",
    "This initiative not only supports the academic goals of content optimization but also fosters collaboration among instructors. By aligning course objectives and minimizing redundancies, educators can offer more diverse and impactful learning experiences. This project serves as a stepping stone for broader efforts to evaluate and enhance curriculum design, ultimately contributing to higher standards in education and better preparing students for the challenges of an ever-evolving professional landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out this project, a step-by-step approach was followed to properly analyze the content similarity between two courses related to \"Artificial Intelligence\" and \"Data Visualization.\" These steps included data identification, text processing, feature extraction, and similarity analysis. Below is a breakdown of each step:\n",
    "\n",
    "**1. Course Selection**\n",
    "Initially, two courses related to \"Artificial Intelligence\" and \"Data Visualization\" were chosen as the subjects for the similarity analysis. For each of these courses, the corresponding text files were extracted and prepared for processing:\n",
    "\n",
    "Course 1: AI2.docx (Content related to Artificial Intelligence)\n",
    "Course 2: Vir1.docx (Content related to Data Visualization)\n",
    "\n",
    "**2. Definition of Key Terms**\n",
    "Next, a set of key terms related to the topics of each course was defined. These terms were used to identify and compare the similarities between the courses:\n",
    "\n",
    "For the AI course: {'ai', 'machine', 'learning', 'intelligence', 'algorithm', 'data', 'model'}\n",
    "For the Data Visualization course: {'story', 'narrative', 'data', 'visualization', 'insights', 'message', 'emotion'}\n",
    "\n",
    "**3. Text Processing and Cleaning**\n",
    "The content of both text files was processed to remove any noise and irrelevant data. This step involved converting the text into a standard format, making it ready for analysis to ensure the results were accurate.\n",
    "\n",
    "**4. Keyword Analysis**\n",
    "At this stage, the number of key terms found in each course was counted. This count served as a metric for comparing the similarities and differences:\n",
    "\n",
    "In Course 1: 82 AI-related terms and 27 Data Visualization-related terms.\n",
    "In Course 2: 123 AI-related terms and 216 Data Visualization-related terms.\n",
    "\n",
    "**5. Similarity Calculation**\n",
    "Using basic formulas, the similarity between the two courses was calculated. One of the metrics used was the comparison of the frequency of key terms in each course, resulting in a similarity score of 0.2433. This indicates a meaningful degree of similarity between the two courses, suggesting there may be overlap in the teaching of certain concepts.\n",
    "\n",
    "**6. Word Cloud Generation**\n",
    "A Word Cloud was created for each course, visually representing the distribution of key terms in their content. This visualization aids in better understanding the similarities and differences between the courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the first I import my necessary library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import load_files\n",
    "# nltk.download('stopwords')\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import keras\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, average_precision_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and Cleaning\n",
    "In any text analysis task, the first crucial step is text processing and cleaning. This stage ensures that the raw textual data is ready for further analysis, such as keyword extraction, sentiment analysis, or similarity detection. During text processing, we remove unwanted characters, irrelevant information, and perform normalization, so the text can be analyzed in a consistent and structured manner.\n",
    "\n",
    "In this project, text processing and cleaning involved extracting content from .docx files, removing unnecessary numerical data from filenames, and preparing the content by structuring it into a usable format for analysis. This process is vital to ensure that only the relevant text data is considered in subsequent steps of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: AI2.docx - Label: AI\n",
      "Content: \n",
      "OK. So that's a little bit the results. OK, so we are somewhere between intermediate and beginners.\n",
      "Great that you feel confident that you might learn the AI concepts. That's very good to know, and o...\n",
      "----------------------------------------\n",
      "Filename: Vir1.docx - Label: Vir\n",
      "Content: \n",
      "Mine too, but last week was I was in Oxford for a seminar and had to prepare the courses.\n",
      "So this was quite stressful and it seems this semester for the new students is even more confusion with getti...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Extracting Content and Labels from DOCX Files\n",
    "import os\n",
    "import docx\n",
    "import re\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to remove numbers from the filename and assign a label\n",
    "def get_label_from_filename(filename):\n",
    "    # Remove digits from the filename using regex\n",
    "    clean_name = re.sub(r'\\d+', '', filename)\n",
    "    # Remove file extension (.docx)\n",
    "    clean_name = clean_name.replace('.docx', '').strip()\n",
    "    return clean_name\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read all docx files from the directory and assign a label based on the filename\n",
    "texts = []\n",
    "labels = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(text)\n",
    "        filenames.append(filename)\n",
    "        \n",
    "        # Extract label from the filename without numbers\n",
    "        label = get_label_from_filename(filename)\n",
    "        labels.append(label)\n",
    "\n",
    "# Display results\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Filename: {filenames[i]} - Label: {labels[i]}\")\n",
    "    print(f\"Content: {text[:200]}...\")  # Display the first 200 characters of the text for brevity\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we performed several preprocessing steps to prepare the text data for similarity analysis. The preprocessing steps we implemented include:\n",
    "**(Similarity between AI2.docx and Vir1.docx: 0.8910 and it is too much)**\n",
    "\n",
    "Converting text to lowercase: This step ensured that text comparison was case-insensitive.\n",
    "\n",
    "Removing numbers: Digits were removed to focus solely on the textual content.\n",
    "\n",
    "Removing punctuation: This helped in standardizing the text and reducing noise.\n",
    "\n",
    "While these preprocessing steps were helpful, we identified areas where improvements were necessary to achieve a more accurate similarity assessment. To enhance our approach, we implemented the following additional steps:\n",
    "\n",
    "Removing stop words: Commonly used words like \"the,\" \"and,\" or \"is,\" which do not contribute significantly to the meaning, were removed to reduce their impact on the similarity calculation.\n",
    "\n",
    "Applying lemmatization or stemming: This step involved reducing words to their base or root form (e.g., \"running\" to \"run\"), allowing us to treat different forms of the same word as identical.\n",
    "\n",
    "Focusing on key terms: We prioritized words related to the specific topics of interest (e.g., \"AI,\" \"algorithm,\" \"model,\" \"visualization\") to limit the analysis to relevant content.\n",
    "\n",
    "Handling repetitive sections: Repeated phrases or structural similarities across documents were addressed to prevent inflated similarity scores.\n",
    "Exploring semantic similarity: Advanced methods, such as using semantic models (e.g., BERT or Word2Vec), were considered to account for contextual meanings rather than solely relying on word matching.\n",
    "\n",
    "These additional steps were crucial in refining the preprocessing process and ensuring that the similarity analysis yielded meaningful and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between AI2.docx and Vir1.docx: 0.8910\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation and convert to lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# List to store the text content of the files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "# Read all docx files from the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Compute TF-IDF vectors for the texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Compute cosine similarity between the first two documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display cosine similarity between the files\n",
    "for i, filename in enumerate(filenames):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"Similarity between {filenames[i]} and {filenames[j]}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this code is to compare two text documents to identify **common words** and **common bigrams (sequences of two consecutive words)** between them. The code aims to provide a better understanding of the overlap and similarity between the two documents. The main steps and objectives are as follows:\n",
    "\n",
    "1. **Reading and Preprocessing Files:**\n",
    "   - Text files (in `.docx` format) are read from a specified directory.\n",
    "   - The content of each file is extracted and preprocessed:\n",
    "     - Converted to lowercase (case normalization).\n",
    "     - Numbers and punctuation are removed.\n",
    "   - The goal of preprocessing is to standardize the text for more accurate analysis.\n",
    "\n",
    "2. **Tokenizing the Text:**\n",
    "   - The text of each file is split into a list of words (tokens).\n",
    "   - This step enables the identification of repeated and common words.\n",
    "\n",
    "3. **Finding Common Words and Calculating Frequency:**\n",
    "   - Common words between the two documents are identified using set intersection (`&`).\n",
    "   - The frequency of each common word in both files is calculated and displayed.\n",
    "\n",
    "4. **Identifying Common Bigrams (Two Consecutive Words):**\n",
    "   - Each text is transformed into a sequence of bigrams (pairs of consecutive words).\n",
    "   - Common bigrams between the two documents are extracted and displayed.\n",
    "   - Examining bigrams helps identify recurring structures or phrases between the texts.\n",
    "\n",
    "\n",
    "The code is designed to analyze the similarity between two text documents in terms of content and structure. The results can be used to evaluate thematic overlap, detect potential repetitions, or refine textual content.\n",
    "\n",
    "## **Reasons for High Similarities:**\n",
    "If there are many common words or bigrams between the documents, it may be due to the following reasons:\n",
    "1. **Similar Topics:** Both documents might cover the same subject matter.\n",
    "2. **Repetitive Phrases:** Standardized phrases or templates may have been used in both files.\n",
    "3. **Insufficient Preprocessing:** Non-essential words (e.g., \"the,\" \"is\") were not removed, affecting the similarity results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Function to read content from a docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to preprocess text (remove punctuation and convert to lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory path containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read all docx files from the directory\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Get the first two documents for comparison\n",
    "text1 = texts[0]\n",
    "text2 = texts[1]\n",
    "\n",
    "# Tokenize the texts into words\n",
    "words1 = text1.split()\n",
    "words2 = text2.split()\n",
    "\n",
    "# Find common words between the two texts\n",
    "common_words = set(words1) & set(words2)\n",
    "\n",
    "# Get the frequency of common words in both texts\n",
    "freq1 = Counter(words1)\n",
    "freq2 = Counter(words2)\n",
    "\n",
    "# Display the common words and their frequencies\n",
    "print(f\"Common words between {filenames[0]} and {filenames[1]}:\")\n",
    "for word in common_words:\n",
    "    print(f\"Word: {word} - Frequency in {filenames[0]}: {freq1[word]} - Frequency in {filenames[1]}: {freq2[word]}\")\n",
    "\n",
    "# Optionally: find common bigrams (sequences of two words)\n",
    "def get_bigrams(text):\n",
    "    words = text.split()\n",
    "    return [' '.join(pair) for pair in zip(words[:-1], words[1:])]\n",
    "\n",
    "bigrams1 = get_bigrams(text1)\n",
    "bigrams2 = get_bigrams(text2)\n",
    "\n",
    "common_bigrams = set(bigrams1) & set(bigrams2)\n",
    "\n",
    "print(f\"\\nCommon bigrams between {filenames[0]} and {filenames[1]}:\")\n",
    "for bigram in common_bigrams:\n",
    "    print(bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What This Code Shows:**\n",
    "\n",
    "This code applies **Latent Dirichlet Allocation (LDA)**, a topic modeling technique, to identify the underlying topics within the provided `.docx` files. The output, which lists the **top words for each topic**, reveals the key terms most strongly associated with each identified topic.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **Topic 1:** ['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
    "- **Topic 2:** ['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']\n",
    "\n",
    "These words represent the **essence of the topics** extracted by the model from the provided documents. \n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose and Use for Subsequent Steps:**\n",
    "\n",
    "1. **Understanding Textual Themes:**\n",
    "   - The output helps us understand the primary themes or topics present in the documents. For instance:\n",
    "     - **Topic 1** seems to focus on conversational terms, possibly indicating informal communication or brainstorming.\n",
    "     - **Topic 2** includes terms like \"AI\" and \"decision,\" which might suggest discussions related to artificial intelligence and decision-making.\n",
    "\n",
    "2. **Refining Text Processing Pipelines:**\n",
    "   - By analyzing these topics, we can identify noise in the text (e.g., filler words like \"yeah,\" \"maybe,\" \"ok,\" and \"just\").\n",
    "   - This insight can guide further preprocessing, such as customizing the stop-word list to exclude irrelevant terms and focus on more meaningful content.\n",
    "\n",
    "3. **Basis for Text Classification or Comparison:**\n",
    "   - Extracted topics can serve as features for comparing documents, grouping similar texts, or even training classification models if labels are available.\n",
    "\n",
    "4. **Improving Alignment With Previous Results:**\n",
    "   - In earlier steps, we identified word frequencies, common words, and bigrams. This approach builds on those analyses by summarizing the **conceptual structure** of the text, helping to better understand similarities and differences between documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use This After Previous Code:**\n",
    "- The previous steps analyzed **word-level overlap** (e.g., shared words and phrases). However, they did not provide a high-level understanding of the themes or topics.\n",
    "- LDA adds a layer of abstraction by grouping related words into topics, giving a clearer picture of the documents' **semantic content**.\n",
    "- This approach ensures we’re not just comparing superficial textual similarities but also deeper thematic alignments, which is crucial for making informed decisions in downstream tasks.\n",
    "\n",
    " this code helps identify the main topics in the documents, providing a thematic overview that can be used to refine the preprocessing pipeline and guide further analyses. but as you can see the resault is not good and we decided change the code and check the other ways.\n",
    "\n",
    " Topic 1:\n",
    "['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
    "Topic 2:\n",
    "['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
      "Topic 2:\n",
      "['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to read and preprocess docx content\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Assuming 2 main topics\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparison Between the Two Codes**\n",
    "\n",
    "Both codes aim to extract **topics** from `.docx` files using **Latent Dirichlet Allocation (LDA)**, but there are significant differences in their **preprocessing steps** and the resulting outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences:**\n",
    "\n",
    "1. **Custom Stopwords:**\n",
    "   - **Previous Code:** Relied only on the built-in English stopword list provided by `CountVectorizer`.\n",
    "   - **Current Code:** Adds a **custom stopword list** (`['yeah', 'ok', 'umm', 'just', 'like', 'bit', 'maybe', 'right', 'thats']`) to filter out common filler words that do not contribute to meaningful topics.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - **Previous Code:** \n",
    "     - Lowercased text.\n",
    "     - Removed digits and punctuation.\n",
    "   - **Current Code:** \n",
    "     - Includes all previous preprocessing steps.\n",
    "     - Additionally removes **custom filler words**, making the data cleaner and more topic-focused.\n",
    "\n",
    "3. **Output Topics:**\n",
    "   - **Previous Code:**\n",
    "     - **Topic 1:** ['yeah', 'ok', 'just', 'think', 'right', 'data', 'like', 'maybe', 'bit', 'umm']\n",
    "     - **Topic 2:** ['thats', 'ai', 'things', 'probably', 'say', 'yeah', 'decision', 'like', 'maybe', 'ok']\n",
    "     - These topics contain numerous filler words (e.g., \"yeah,\" \"ok,\" \"maybe\") that obscure the main themes.\n",
    "   - **Current Code:**\n",
    "     - **Topic 1:** ['ai', 'things', 'probably', 'say', 'decision', 'kind', 'human', 'actually', 'basically', 'different']\n",
    "     - **Topic 2:** ['think', 'data', 'lets', 'know', 'say', 'income', 'dont', 'course', 'really', 'want']\n",
    "     - The removal of filler words results in more **coherent topics**, highlighting terms related to artificial intelligence, decision-making, and data.\n",
    "\n",
    "4. **Clarity of Topics:**\n",
    "   - The **current code** generates more focused topics, making them easier to interpret and align with the context of the documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Results:**\n",
    "\n",
    "### **Previous Results:**\n",
    "- Contained many **irrelevant filler words**.\n",
    "- Topic differentiation was less clear due to noise in the data.\n",
    "\n",
    "### **Current Results:**\n",
    "- More meaningful and specific:\n",
    "  - **Topic 1** emphasizes AI-related terms like \"ai,\" \"decision,\" and \"different.\"\n",
    "  - **Topic 2** relates to discussions about data analysis, with terms like \"data,\" \"income,\" and \"course.\"\n",
    "- The removal of custom stopwords improved **topic clarity**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why the Current Code Was Used After the Previous One:**\n",
    "\n",
    "1. **Improvement in Preprocessing:**\n",
    "   - Observing the prevalence of filler words in the previous output indicated a need for additional filtering. Adding a **custom stopword list** addressed this issue.\n",
    "\n",
    "2. **Enhancing Interpretability:**\n",
    "   - By focusing on meaningful words, the current approach provides **clearer and more actionable insights** about the themes in the documents.\n",
    "\n",
    "3. **Aligning Topics with Goals:**\n",
    "   - The improved topics can be better leveraged for tasks like document comparison, classification, or deeper semantic analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **Use in Subsequent Steps:**\n",
    "- The clearer topics can now serve as:\n",
    "  - **Features for clustering documents** into related groups.\n",
    "  - **A guide to refine further preprocessing** for different datasets.\n",
    "  - **Basis for thematic analysis** or comparisons between texts.\n",
    "\n",
    "the current code improves on the previous one by eliminating noise, generating cleaner topics, and enabling more focused analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['ai', 'things', 'probably', 'say', 'decision', 'kind', 'human', 'actually', 'basically', 'different']\n",
      "Topic 2:\n",
      "['think', 'data', 'lets', 'know', 'say', 'income', 'dont', 'course', 'really', 'want']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to read and preprocess docx content\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Custom stopwords list to remove common filler words\n",
    "custom_stopwords = set(['yeah', 'ok', 'umm', 'just', 'like', 'bit', 'maybe', 'right', 'thats'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in custom_stopwords])\n",
    "    return text\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Adjust n_components based on your data\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation of the Code:**\n",
    "\n",
    "This version introduces an **advanced preprocessing step** by incorporating **Named Entity Recognition (NER)** using SpaCy. This enhances the clarity of topics extracted by the Latent Dirichlet Allocation (LDA) model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Features of the Code:**\n",
    "\n",
    "1. **Named Entity Recognition (NER) Filtering:**\n",
    "   - The code uses SpaCy's `en_core_web_sm` model to identify and remove named entities (e.g., names, places, dates, organizations) from the text.\n",
    "   - By excluding these entities, the focus shifts to general terms and patterns, reducing noise caused by specific names or details.\n",
    "\n",
    "2. **Stopword Customization:**\n",
    "   - The code expands the custom stopword list to include more filler words such as `'say', 'lets', 'think', 'know', 'course', 'really', 'probably'`.\n",
    "   - This refinement ensures that the resulting topics are not dominated by irrelevant or generic terms.\n",
    "\n",
    "3. **Enhanced Preprocessing Pipeline:**\n",
    "   - Converts text to lowercase.\n",
    "   - Removes digits and punctuation.\n",
    "   - Filters out named entities and stopwords.\n",
    "\n",
    "4. **Topic Modeling:**\n",
    "   - The vectorization step (`CountVectorizer`) converts the cleaned text into a **document-term matrix**.\n",
    "   - **Latent Dirichlet Allocation (LDA)** is applied to extract two topics (`n_components=2`).\n",
    "   - The top 10 terms for each topic are displayed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Previous Codes:**\n",
    "\n",
    "1. **Addition of NER:**\n",
    "   - In previous versions, named entities (like \"AI,\" \"human,\" \"decision\") were included in the topic extraction process. While these entities might occasionally be relevant, their overrepresentation could obscure broader patterns.\n",
    "   - The current code removes such entities to focus on more general and **conceptual terms**.\n",
    "\n",
    "2. **Improved Topic Clarity:**\n",
    "   - By removing named entities, the extracted topics emphasize broader patterns instead of being skewed by document-specific details.\n",
    "\n",
    "3. **Expanded Stopwords List:**\n",
    "   - The expanded stopword list further reduces noise and highlights meaningful words.\n",
    "\n",
    "---\n",
    "\n",
    "## **Expected Results:**\n",
    "\n",
    "- **Topics from Previous Code:**\n",
    "  - **Topic 1:** ['ai', 'things', 'probably', 'say', 'decision', 'kind', 'human', 'actually', 'basically', 'different']\n",
    "  - **Topic 2:** ['think', 'data', 'lets', 'know', 'say', 'income', 'dont', 'course', 'really', 'want']\n",
    "\n",
    "- **Topics from Current Code:**\n",
    "  - The new topics are expected to:\n",
    "    - Exclude specific names and entities.\n",
    "    - Highlight key themes or terms related to the general context of the documents.\n",
    "    - Example: [\"decision-making,\" \"data analysis,\" \"technological impact\"].\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of This Code for Future Steps:**\n",
    "\n",
    "1. **Generalization:**\n",
    "   - Topics derived from this process are likely to generalize better across different datasets since named entities and filler words are removed.\n",
    "\n",
    "2. **Suitability for Downstream Tasks:**\n",
    "   - The output can be used for:\n",
    "     - **Document classification or clustering.**\n",
    "     - **Thematic comparison** across files.\n",
    "     - **Keyword extraction** for summarization.\n",
    "\n",
    "3. **Focused Analysis:**\n",
    "   - By eliminating unnecessary noise, this method lays a stronger foundation for deeper text analysis or comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Was This Update Introduced?**\n",
    "\n",
    "- To address **entity-specific noise** in the previous results.\n",
    "- To enable broader **generalization** and clearer topics by eliminating unnecessary details.\n",
    "- To prepare the data for future analyses that require a **higher-level understanding** of document themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['data', 'nt', 'income', 'things', 'people', 'want', 'time', 'look', 'lot', 'different']\n",
      "Topic 2:\n",
      "['ai', 'decision', 'based', 'aspects', 'language', 'database', 'autonomous', 'driving', 'intelligence', 'ultimately']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "\n",
    "# Load Spacy model for NER (Named Entity Recognition)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to read and preprocess docx content\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Custom stopwords list to remove common filler words\n",
    "custom_stopwords = set(['yeah', 'ok', 'umm', 'just', 'like', 'bit', 'maybe', 'right', 'thats', 'say', 'lets', 'think', 'know', 'course', 'really', 'probably'])\n",
    "\n",
    "# Function to preprocess text and remove named entities (NER)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in custom_stopwords])\n",
    "\n",
    "    # Use Spacy to remove named entities\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([token.text for token in doc if not token.ent_type_])  # Remove named entities\n",
    "    return text\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Adjust n_components based on your data\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    wn_pos_tags = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = []\n",
    "    for token, pos in pos_tags:\n",
    "        pos = wn_pos_tags.get(pos[0].upper(), 'n')  # If the POS tag is not recognized, default to noun (n)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing in a single code\n",
    "#import nltk\n",
    "#nltk.download('words')\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "def text_cleaner(contents):\n",
    "    contents = \" \".join(filter(lambda x: x[0]!= '@' , contents.split())) # removes any word which starts with @.\n",
    "    contents = re.sub('[^0-9a-zA-Z]', ' ', contents) # substitute all non-alphabetic characters in the string with spaces.\n",
    "    contents = contents.lower() # lowercasing\n",
    "    contents = re.sub(' +', ' ', contents).strip() # replacing more than one space with 1 space and remove any leading and trailing space.\n",
    "    contents = nltk.wordpunct_tokenize(contents)\n",
    "    contents = [word for word in contents if not word.isdigit()]\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]\n",
    "    contents = [word for word in contents if not word in set(stopwords.words('english'))] # Keeping only the non-stop words in the string\n",
    "    contents = lemmatize(contents)\n",
    "    contents = \" \".join(contents)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code introduces a **comprehensive text preprocessing pipeline** which performs various text cleaning and preparation steps before further analysis. Here’s a detailed breakdown of what this code does:\n",
    "\n",
    "## **Key Features of the Code:**\n",
    "\n",
    "1. **Text Cleaning with `text_cleaner` Function:**\n",
    "   - **Remove `@` symbols:** Any word starting with `@` is removed, likely to exclude social media mentions.\n",
    "   - **Substitute non-alphabetic characters:** It replaces any non-alphabetic characters (like punctuation and special characters) with spaces.\n",
    "   - **Lowercasing:** All text is converted to lowercase to standardize it.\n",
    "   - **Remove extra spaces:** Extra spaces are collapsed into a single space, and leading/trailing spaces are removed.\n",
    "   - **Tokenization:** It breaks the text into individual words (tokens).\n",
    "   - **Remove digits and non-alphabetic words:** Any digits or non-alphabetic words are removed.\n",
    "   - **Remove stopwords:** The list of common English stopwords (like \"the\", \"and\", \"is\") is excluded from the text.\n",
    "   - **Lemmatization:** Words are lemmatized (reduced to their base form), considering their part-of-speech (POS).\n",
    "   \n",
    "2. **Lemmatization with POS Tagging:**\n",
    "   - Each word is tagged with its **Part of Speech (POS)**, and based on the POS, the correct lemmatization process is applied.\n",
    "   - Words are lemmatized into their root form (e.g., \"running\" becomes \"run\", \"better\" becomes \"good\").\n",
    "\n",
    "3. **Reading .docx Files:**\n",
    "   - **`read_docx` function** reads the content of `.docx` files, extracting the text from all paragraphs and concatenating them.\n",
    "\n",
    "4. **Iterating through Files in a Directory:**\n",
    "   - The script iterates through all files in a given directory (`directory` variable), and for each `.docx` file, it reads and preprocesses the text.\n",
    "   - The cleaned text is then stored in a list (`texts`), and filenames are stored in `filenames`.\n",
    "\n",
    "5. **Displaying Cleaned Text:**\n",
    "   - The cleaned text for each file is printed for verification, so you can visually check how the text has been processed.\n",
    "\n",
    "## **Function Breakdown:**\n",
    "\n",
    "- **`text_cleaner` function** performs the core preprocessing:\n",
    "  - Removes words starting with `@`.\n",
    "  - Strips out non-alphabetical characters.\n",
    "  - Tokenizes, removes digits, filters valid words, and removes stopwords.\n",
    "  - Lemmatizes the remaining words and reassembles the cleaned text.\n",
    "\n",
    "- **`lemmatize` function** uses POS tagging to apply the correct lemmatization based on the word’s part of speech (noun, verb, adjective, etc.).\n",
    "\n",
    "## **Expected Output:**\n",
    "\n",
    "For each `.docx` file in the specified directory, the program will output the cleaned text. The cleaned text will have:\n",
    "- No special characters, digits, or stopwords.\n",
    "- All words lemmatized to their base forms (e.g., \"running\" becomes \"run\").\n",
    "- Words such as \"better\" would be lemmatized to \"good.\"\n",
    "\n",
    "Here’s an example of what the output might look like:\n",
    "\n",
    "```\n",
    "Cleaned text from document1.docx:\n",
    "this study focus on analysis of data from various sensor to predict future trends in health care.\n",
    "----------------------------------------\n",
    "Cleaned text from document2.docx:\n",
    "ai technology is becoming increasingly important in decision making across multiple industries.\n",
    "----------------------------------------\n",
    "```\n",
    "\n",
    "## **Why This Code is Useful:**\n",
    "\n",
    "- **Data Preprocessing for NLP Models:** The cleaned text is ready for further natural language processing tasks such as topic modeling, sentiment analysis, or text classification.\n",
    "- **Consistent Format:** By removing stopwords, digits, special characters, and applying lemmatization, the text becomes standardized, reducing noise and improving the accuracy of downstream models.\n",
    "- **Scalability:** This method works for any number of `.docx` files, and it's easy to modify if you need to preprocess other types of files.\n",
    "\n",
    "## **Potential Improvements/Modifications:**\n",
    "- **Custom Stopwords List:** You might want to expand or modify the stopword list to better suit your specific dataset.\n",
    "- **POS Tagging Improvement:** You can fine-tune the POS tagging process or integrate a more sophisticated lemmatizer if needed for specialized vocabularies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import docx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Make sure to download the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    contents = \" \".join(filter(lambda x: x[0] != '@', contents.split()))  # Remove words starting with @\n",
    "    contents = re.sub('[^0-9a-zA-Z]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "    contents = contents.lower()  # Lowercase\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "    contents = nltk.word_tokenize(contents)  # Tokenization\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep only valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]  # Lemmatization\n",
    "    contents = \" \".join(contents)  # Join back to string\n",
    "    return contents\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Display cleaned texts for verification\n",
    "for filename, cleaned_text in zip(filenames, texts):\n",
    "    print(f\"Cleaned text from {filename}:\")\n",
    "    print(cleaned_text)\n",
    "    print(\"----\" * 10)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Make sure to download the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove any patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "    contents = contents.lower()  # Lowercase\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "    contents = nltk.word_tokenize(contents)  # Tokenization\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep only valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]  # Lemmatization\n",
    "    contents = \" \".join(contents)  # Join back to string\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure that you have downloaded the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove any patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words (customize as needed)\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words and alphanumeric combinations\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove alphanumeric combinations that don't match meaningful patterns (e.g., \"ai4bs\")\n",
    "    contents = [word for word in contents if not re.match(r'^[a-zA-Z]+\\d+[a-zA-Z]*$', word)]  # e.g., ai4bs\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code I've provided is well-structured for cleaning and preprocessing text data, particularly from `.docx` files. It removes unnecessary parts of the text (e.g., session details, timestamps), performs tokenization, and filters out stop words, digits, meaningless words, and duplicates. Additionally, it performs lemmatization to ensure that the words are reduced to themy base forms.\n",
    "\n",
    "Here's a breakdown of how the code works:\n",
    "\n",
    "1. **Text Cleaning (Regex Replacements):**\n",
    "   - It removes specific unwanted patterns (e.g., class sessions, AM/PM timestamps) using regular expressions (`re.sub`).\n",
    "   - Non-alphabetic characters are replaced with spaces (`re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)`), and multiple spaces are reduced to a single space.\n",
    "\n",
    "2. **Tokenization and Filtering:**\n",
    "   - Tokenizes the text into words using `nltk.word_tokenize`.\n",
    "   - Filters out digits, invalid words, and stop words.\n",
    "   - It also removes custom meaningless words (`4bs`, `f`, `xh`) and patterns like alphanumeric combinations (e.g., \"ai4bs\").\n",
    "\n",
    "3. **Lemmatization:**\n",
    "   - Uses `WordNetLemmatizer` from NLTK to convert words to themy base form (e.g., \"running\" to \"run\").\n",
    "\n",
    "4. **File Handling:**\n",
    "   - Reads `.docx` files using the `docx` library.\n",
    "   - For each file, the `text_cleaner` function is applied to preprocess the content.\n",
    "\n",
    "5. **Dmyectory Traversal:**\n",
    "   - Reads all `.docx` files from a specified dmyectory, processes them, and stores the cleaned content in a list.\n",
    "\n",
    "Finally, the cleaned text from each file is printed for verification.\n",
    "\n",
    "## Potential Improvements:\n",
    "- **Error Handling:** It might be useful to add error handling for cases where the `.docx` files cannot be read or processed correctly.\n",
    "- **Efficiency:** If the dmyectory contains many files, I could consider processing the files in parallel or batch processing to speed up execution.\n",
    "\n",
    "\n",
    "the code works as expected, I see the cleaned and processed text output from each `.docx` file in my dmyectory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Display cleaned texts for verification\n",
    "for filename, cleaned_text in zip(filenames, texts):\n",
    "    print(f\"Cleaned text from {filename}:\")\n",
    "    print(cleaned_text)\n",
    "    print(\"----\" * 10)  # Separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I decided to use the textual data from the .docx files. I started by reading the files and preprocessing the text using the preprocess_text function. The goal in this step was to clean up the text by removing noise such as punctuation or irrelevant words, converting the text into a simpler form that would be more suitable for analytical models.\n",
    "\n",
    "After preprocessing the texts, I applied Topic Modeling to analyze the data. For this task, I used the Latent Dirichlet Allocation (LDA) model, which helped me identify different topics within the texts. To begin, I converted the texts into a document-term matrix using CountVectorizer, where each row represented a document and each column represented a word.\n",
    "\n",
    "To ensure that the model accurately identified topics, I set the number of topics to 2 (this number can be adjusted based on the data). I selected this number because I wanted the model to extract two primary categories of concepts.\n",
    "\n",
    "After running the model, I ended up with the following topics:\n",
    "\n",
    "Topic 1:\n",
    "\n",
    "['data', 'nt', 'income', 'things', 'people', 'want', 'time', 'look', 'lot', 'different']\n",
    "Topic 1 is related to concepts such as \"data,\" \"income,\" and \"time.\" It seems to refer to the analysis of data and its use in various contexts.\n",
    "\n",
    "Topic 2:\n",
    "\n",
    "['ai', 'decision', 'based', 'aspects', 'language', 'database', 'autonomous', 'driving', 'intelligence', 'ultimately']\n",
    "Topic 2 focuses on concepts like \"artificial intelligence,\" \"decision-making,\" and \"autonomous driving.\" This topic likely relates to applications of AI and related technologies.\n",
    "\n",
    "Initially, I encountered some issues with the previous code, so I decided to implement this new approach to preprocessing and analyzing the texts. With this new method, I was able to identify the main topics present in the data and obtain meaningful results that aid in a better understanding of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['data', 'nt', 'income', 'things', 'people', 'want', 'time', 'look', 'lot', 'different']\n",
      "Topic 2:\n",
      "['ai', 'decision', 'based', 'aspects', 'language', 'database', 'autonomous', 'driving', 'intelligence', 'ultimately']\n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        texts.append(preprocess_text(text))  # Preprocess the text\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Vectorize the texts (convert text into word count vectors)\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Adjust n_components based on your data\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how I arrived at the difference in similarity scores between 89% and 47%, and the reasoning behind the process:\n",
    "\n",
    "Initially, I started by reading and preprocessing the text data from all the .docx files. The goal was to clean the data, removing unwanted content (like session details or timestamps) and irrelevant words. This is where I focused on eliminating stop words, non-alphabetic characters, and some meaningless words like \"4bs\" or \"f.\" I also performed lemmatization to normalize the words, ensuring that they appeared in their base forms (e.g., changing \"running\" to \"run\").\n",
    "\n",
    "Once the preprocessing was done, I used TF-IDF (Term Frequency-Inverse Document Frequency), which is a method for transforming the text data into numerical vectors. This vectorization allows us to represent the documents in a way that makes it easier to calculate similarities between them.\n",
    "\n",
    "For the cosine similarity step, I compared each document's TF-IDF vector to the others. Cosine similarity measures how close two vectors are, meaning it looks at how similar the content is between the documents. The score ranges from 0 (completely different) to 1 (identical).\n",
    "\n",
    "For instance, I initially got a similarity score of 89% between two documents (let's call them AI2.docx and Vir1.docx). However, when revisiting the comparison later, I found a lower similarity score of 47%. This difference can be attributed to several factors:\n",
    "\n",
    "Preprocessing: During the text cleaning process, some important words might have been removed, or the lemmatization might have altered the meaning slightly, which could have affected the final similarity scores. It's possible that in one iteration, the preprocessing steps removed more useful words, leading to a reduced similarity score.\n",
    "\n",
    "TF-IDF Vectorization: The TF-IDF transformation is sensitive to the presence of rare terms and their importance in a document. If two documents have more unique terms that are not shared between them, the similarity score will be lower. It's likely that in one of the calculations, the key terms that defined the similarity between the documents were weighted differently, resulting in a significant drop.\n",
    "\n",
    "Document Content: If the content of the documents is slightly varied, the similarity score will reflect that. For example, documents with highly overlapping words (like \"ai,\" \"data,\" \"income,\" \"decision\") would have a higher cosine similarity, but if one document introduced more specific or different words, the similarity score could decrease.\n",
    "\n",
    "Ranking of Keywords: The top keywords also play a crucial role. The keywords for AI2.docx include terms like \"ai,\" \"decision,\" and \"probably,\" which are related to decision-making and artificial intelligence. On the other hand, the keywords for Vir1.docx focus more on terms like \"yeah,\" \"think,\" and \"income,\" which suggest that the documents discuss slightly different topics. This difference in keyword relevance likely contributed to the observed drop in similarity between the two documents.\n",
    "\n",
    "So, while the first result was 89%, this second calculation of 47% reflects a more accurate and nuanced similarity, taking into account the impact of preprocessing, vectorization, and the actual content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'AI2.docx' and 'Vir1.docx': 0.4787\n",
      "\n",
      "Top keywords in 'AI2.docx': ai, decision, probably, say, yeah, like, maybe, kind, human, actually\n",
      "\n",
      "Top keywords in 'Vir1.docx': yeah, think, right, data, income, like, maybe, bit, let, say\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Initialize the lemmatizer and set of English words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove specific unwanted patterns (e.g., session details, timestamps, etc.)\n",
    "    contents = re.sub(r'\\b(?:class|session|meeting|recording|AM|PM)\\b.*?[\\d:]+.*?(\\s|$)', '', contents, flags=re.IGNORECASE)\n",
    "    contents = re.sub(r'\\b\\d+\\s*AM\\s*\\d+h\\s*\\d+m\\s*\\d+s\\b', '', contents)  # Remove patterns like \"23am 1h 13m 22s\"\n",
    "    contents = re.sub(r'[^0-9a-zA-Z\\s]', ' ', contents)  # Substitute non-alphabetic characters with spaces\n",
    "\n",
    "    # Lowercase and replace multiple spaces with a single space\n",
    "    contents = contents.lower()  \n",
    "    contents = re.sub(' +', ' ', contents).strip()  \n",
    "\n",
    "    # Tokenization\n",
    "    contents = nltk.word_tokenize(contents)\n",
    "\n",
    "    # Remove digits, meaningless words, and duplicates\n",
    "    contents = [word for word in contents if not word.isdigit()]  # Remove digits\n",
    "    contents = [word for word in contents if word.lower() in words or not word.isalpha()]  # Keep valid words\n",
    "    contents = [word for word in contents if word not in set(stopwords.words('english'))]  # Remove stop words\n",
    "\n",
    "    # Remove meaningless words and alphanumeric combinations\n",
    "    meaningless_words = {'4bs', 'f', 'xh'}\n",
    "    contents = [word for word in contents if word not in meaningless_words]\n",
    "\n",
    "    # Remove alphanumeric combinations that don't match meaningful patterns (e.g., \"ai4bs\")\n",
    "    contents = [word for word in contents if not re.match(r'^[a-zA-Z]+\\d+[a-zA-Z]*$', word)]  # e.g., ai4bs\n",
    "\n",
    "    # Remove consecutive duplicates\n",
    "    contents = [word for i, word in enumerate(contents) if i == 0 or word != contents[i - 1]]\n",
    "\n",
    "    # Lemmatization\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "\n",
    "    # Join back to string\n",
    "    contents = \" \".join(contents)\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)  # Use the text_cleaner function\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display similarity scores\n",
    "for i in range(len(filenames)):\n",
    "    for j in range(i + 1, len(filenames)):\n",
    "        print(f\"Similarity between '{filenames[i]}' and '{filenames[j]}': {similarity_matrix[i][j]:.4f}\")\n",
    "\n",
    "# If you want to extract keywords from each document based on TF-IDF\n",
    "n_keywords = 10  # Number of top keywords to display\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    sorted_indices = tfidf_matrix[i].toarray()[0].argsort()[::-1][:n_keywords]\n",
    "    keywords = [feature_names[idx] for idx in sorted_indices]\n",
    "    print(f\"\\nTop keywords in '{filename}': {', '.join(keywords)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Steps Taken:\n",
    "1. **Text Preprocessing**: \n",
    "   - First, I read the content of each `.docx` file using the `read_docx` function and extracted the text.\n",
    "   - Then, I cleaned the extracted text by removing non-alphabetic characters and converting everything to lowercase, so that the focus was only on words without being affected by case sensitivity or non-alphabetical symbols.\n",
    "   - After cleaning, I split the text into individual words using the `split()` function.\n",
    "\n",
    "2. **Counting Keywords**:\n",
    "   - I defined two sets of keywords: one for **AI** (`keywords_ai`) and one for **vir** (`keywords_vir`).\n",
    "   - For each word in the cleaned text, I counted how many times words from each keyword set (AI and vir) appeared. This helped me measure the presence of AI-related and vir-related terms in each document.\n",
    "\n",
    "## Results:\n",
    "- **File: 'AI2.docx'**\n",
    "  - AI Keywords: 82\n",
    "  - Vir Keywords: 29\n",
    "  \n",
    "  The `AI2.docx` file contains 82 occurrences of AI-related keywords and 29 occurrences of vir-related keywords. This indicates that the document focuses more on AI topics but also touches on vir-related terms.\n",
    "\n",
    "- **File: 'Vir1.docx'**\n",
    "  - AI Keywords: 123\n",
    "  - Vir Keywords: 123\n",
    "  \n",
    "  In the case of the `Vir1.docx` file, both AI and vir-related keywords appear equally, with 123 occurrences for each. This indicates that the document equally addresses both AI and vir topics.\n",
    "\n",
    "## How I Reached This:\n",
    "1. **Text Extraction**: I started by extracting the content of the `.docx` files using the `python-docx` library.\n",
    "2. **Text Cleaning**: Then, I cleaned the text by removing unnecessary characters and splitting it into words.\n",
    "3. **Keyword Comparison**: Finally, I compared the content of each document with the keyword sets for AI and vir, counting how many times each set of keywords appeared.\n",
    "\n",
    "This method helps me quickly identify which topic—AI or vir—is more prevalent in each document and where the primary focus lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 'AI2.docx', AI Keywords: 82, vir Keywords: 29\n",
      "File: 'Vir1.docx', AI Keywords: 123, vir Keywords: 123\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define specialized keywords for each topic\n",
    "keywords_ai = {'ai', 'machine', 'learning', 'intelligence', 'algorithm', 'data', 'model'}\n",
    "keywords_vir = {'machine', 'learning', 'data', 'predict', 'regression', 'classification', 'algorithm'}\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    contents = re.sub(r'[^a-zA-Z\\s]', '', contents).lower()\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "\n",
    "    # Tokenization\n",
    "    contents = contents.split()  # Split the string into words\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Compare specialized keywords\n",
    "for i in range(len(texts)):\n",
    "    # Count keywords for AI\n",
    "    ai_count = sum(1 for word in texts[i] if word in keywords_ai)\n",
    "    # Count keywords for vir\n",
    "    vir_count = sum(1 for word in texts[i] if word in keywords_vir)\n",
    "\n",
    "    print(f\"File: '{filenames[i]}', AI Keywords: {ai_count}, vir Keywords: {vir_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this code, you are calculating the similarity between documents based on the occurrence of specialized keywords related to **AI** and **vir** (visualization and storytelling) topics.\n",
    "\n",
    "## Explanation of the Code:\n",
    "1. **Text Preprocessing**: \n",
    "   - You read each `.docx` file and clean the text by removing non-alphabetical characters and converting the text to lowercase.\n",
    "   - The text is then split into words for analysis.\n",
    "\n",
    "2. **Keyword Matching**:\n",
    "   - You define two sets of keywords:\n",
    "     - **Keywords related to AI**: `keywords_ai`\n",
    "     - **Keywords related to vir**: `keywords_vir`\n",
    "   - For each document pair, you count the number of AI and vir keywords that appear in both documents.\n",
    "\n",
    "3. **Similarity Calculation**:\n",
    "   - For each pair of documents, you calculate the number of shared AI and vir keywords. \n",
    "   - You calculate the **similarity score** as the ratio of the total number of shared keywords (AI and vir) to the total number of keywords (AI + vir) in both documents.\n",
    "\n",
    "## How Similarity is Calculated:\n",
    "1. **Keyword Counts**: \n",
    "   - For each document, you count how many times AI and vir keywords appear.\n",
    "   \n",
    "2. **Shared Keywords**:\n",
    "   - The similarity between two documents is calculated based on the **shared AI** and **shared vir** keywords. The number of shared keywords is the **minimum count** of a keyword in both documents.\n",
    "   \n",
    "## Result:\n",
    "- The similarity score between **'AI2.docx'** and **'Vir1.docx'** is **0.2433**.\n",
    "  - This means that there is a 24.33% overlap in the AI and vir-related content of the two documents based on the defined keywords.\n",
    "\n",
    "## How This Result is Reached:\n",
    "- **Document 1 ('AI2.docx')**: Has 82 occurrences of AI-related keywords and 29 occurrences of vir-related keywords.\n",
    "- **Document 2 ('Vir1.docx')**: Has 123 occurrences of AI-related keywords and 123 occurrences of vir-related keywords.\n",
    "- After counting and comparing the shared AI and vir keywords, the similarity score was computed as **0.2433**, indicating a low but significant overlap in the terms related to both AI and vir between these two documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'AI2.docx' and 'Vir1.docx': 0.2433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define specialized keywords for each topic\n",
    "keywords_ai = {'ai', 'machine', 'learning', 'intelligence', 'algorithm', 'data', 'model'}\n",
    "keywords_vir = {'story','narrative','data','insights','visualization','audience','context','emotion','message','structure','engagement','character','theme',\n",
    "'experience','communicate'}\n",
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def text_cleaner(contents):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    contents = re.sub(r'[^a-zA-Z\\s]', '', contents).lower()\n",
    "    contents = re.sub(' +', ' ', contents).strip()  # Replace multiple spaces with a single space\n",
    "\n",
    "    # Tokenization\n",
    "    contents = contents.split()  # Split the string into words\n",
    "    return contents\n",
    "\n",
    "# Directory containing the .docx files\n",
    "directory = 'C:/Users/Nastaran/Desktop/UniversityConstructor/semester4/MAster/data'\n",
    "\n",
    "# Read and preprocess all docx files\n",
    "texts = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        text = read_docx(file_path)\n",
    "        cleaned_text = text_cleaner(text)\n",
    "        texts.append(cleaned_text)  # Add cleaned text to the list\n",
    "        filenames.append(filename)\n",
    "\n",
    "# Compare specialized keywords and calculate similarity\n",
    "similarity_scores = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        # Count keywords for AI\n",
    "        ai_count_i = sum(1 for word in texts[i] if word in keywords_ai)\n",
    "        ai_count_j = sum(1 for word in texts[j] if word in keywords_ai)\n",
    "\n",
    "        # Count keywords for ML\n",
    "        vir_count_i = sum(1 for word in texts[i] if word in keywords_vir)\n",
    "        vir_count_j = sum(1 for word in texts[j] if word in keywords_vir)\n",
    "\n",
    "        # Calculate similarity based on shared keywords\n",
    "        total_keywords = ai_count_i + ai_count_j + vir_count_i + vir_count_j\n",
    "        shared_ai_keywords = min(ai_count_i, ai_count_j)\n",
    "        shared_vir_keywords = min(vir_count_i, vir_count_j)\n",
    "\n",
    "        if total_keywords > 0:\n",
    "            similarity = (shared_ai_keywords + shared_vir_keywords) / total_keywords\n",
    "        else:\n",
    "            similarity = 0  # No keywords to compare\n",
    "\n",
    "        similarity_scores.append((filenames[i], filenames[j], similarity))\n",
    "\n",
    "# Display similarity scores\n",
    "for file1, file2, score in similarity_scores:\n",
    "    print(f\"Similarity between '{file1}' and '{file2}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Conclusion:\n",
    "In this analysis, you have examined two distinct fields: **Artificial Intelligence (AI)** and **Data Visualization** using specialized keywords for each domain. Through the extraction and processing of keywords, you have generated word clouds that highlight the distinctions and similarities between these two fields.\n",
    "\n",
    "- **Artificial Intelligence (AI)** focuses more on technology, algorithms, and models, with keywords like \"ai\", \"learning\", \"machine\", \"intelligence\", and \"algorithm\".\n",
    "- **Data Visualization** emphasizes conveying information and storytelling through data, with keywords such as \"message\", \"visualization\", \"context\", \"story\", \"audience\", \"insights\", and \"experience\".\n",
    "\n",
    "As a result, you have successfully highlighted the differences between these two domains through their respective keywords, providing a better understanding of both fields and their interconnections.\n",
    "\n",
    "# Suggestions for Next Steps:\n",
    "1. **Expand the Keyword Sets**:\n",
    "   - To enhance the accuracy of your analysis, consider expanding the keyword sets for both domains. For example:\n",
    "     - In the AI domain, you could add terms like \"deep learning\", \"neural networks\", \"automation\", and \"big data\".\n",
    "     - In the Visualization domain, terms like \"interactive\", \"graph\", \"dashboard\", \"chart\", and \"storytelling\" could be included.\n",
    "   \n",
    "   Expanding these keywords will allow for more refined analyses and provide deeper insights into similarities and differences.\n",
    "\n",
    "2. **Use More Advanced Models for Semantic Analysis**:\n",
    "   - Simple keyword-based approaches are effective, but for greater accuracy, consider utilizing more advanced **Natural Language Processing (NLP)** models. Techniques such as **TF-IDF (Term Frequency-Inverse Document Frequency)** or **Word2Vec** can help you capture semantic and conceptual similarities between texts more precisely.\n",
    "   \n",
    "3. **Comparative Analysis and Clustering**:\n",
    "   - For more detailed comparison and grouping of similar texts, consider using **clustering techniques** like **K-means** or **Hierarchical Clustering**. These methods can help you group similar texts together and provide a better understanding of relationships between them.\n",
    "\n",
    "4. **Explore Relationships Between Domains**:\n",
    "   - If you're interested in deeper insights into the relationships between these two fields, you could perform **correlation analysis**. This will allow you to explore how concepts and keywords from one domain may appear in the other and how these domains might influence each other.\n",
    "\n",
    "5. **Apply Machine Learning Models**:\n",
    "   - For a more sophisticated comparison of texts, you could apply **machine learning models** like **neural networks** to analyze the texts. These models can process the data more deeply and uncover complex relationships between texts.\n",
    "\n",
    "## Summary:\n",
    "You have laid a solid foundation for comparing the **AI** and **Data Visualization** domains based on their keywords. For the next steps, it is suggested that you expand your keyword sets, explore more advanced NLP and machine learning models, and conduct deeper semantic and comparative analyses to achieve more comprehensive and meaningful results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
